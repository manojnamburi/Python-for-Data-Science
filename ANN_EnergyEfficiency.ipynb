{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ENB2012_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>22.307201</td>\n",
       "      <td>24.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>10.090196</td>\n",
       "      <td>9.513306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>12.992500</td>\n",
       "      <td>15.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>18.950000</td>\n",
       "      <td>22.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>31.667500</td>\n",
       "      <td>33.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4         X5          X6  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
       "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
       "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
       "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
       "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
       "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
       "\n",
       "               X7         X8          Y1          Y2  \n",
       "count  768.000000  768.00000  768.000000  768.000000  \n",
       "mean     0.234375    2.81250   22.307201   24.587760  \n",
       "std      0.133221    1.55096   10.090196    9.513306  \n",
       "min      0.000000    0.00000    6.010000   10.900000  \n",
       "25%      0.100000    1.75000   12.992500   15.620000  \n",
       "50%      0.250000    3.00000   18.950000   22.080000  \n",
       "75%      0.400000    4.00000   31.667500   33.132500  \n",
       "max      0.400000    5.00000   43.100000   48.030000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7c9d29588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAD8CAYAAACvm7WEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGE1JREFUeJzt3XmQXXWZxvHvQzCApRhADcgiDOBU\ncIskQogSYmM0qAjuxtGBjKbjGCWuiOOAiDVTCCM6FHSZjgsIWmqhpQETjUGD41QidBakQkTiEmlA\nUYnbICUx7/xxTuK10/f0vd33rDyfqlO5y+lzntzuvP3Le5afIgIzM6uGfcoOYGZmf+OibGZWIS7K\nZmYV4qJsZlYhLspmZhXiomxmViEuymZmFeKibGZWIS7KZmYVsm/+u1hc+iWD+nDZCRKLTiw7AXx5\nS9kJEj9+R9kJYOC2shPYSBfPXaaJb6WbmtOL/fWWR8pmZhVSwEjZzKw43fzXvHLDZFyUzaxhdnVR\nlSdVsCq7KJtZo3R140sXZTOzfJV+ZsEEuSibWaPU/RbxLspm1ig1r8kuymbWLB4pm5lVSDdnX1SR\ni7KZNUrNa7KLspk1i9sXZmYVUvOaPL57X0ia1+sgZma9ENH5UkXjvSHRp7PelNQvaUjS0ODg1nHu\nwsyse7ui86WK2rYvJK1o9xZwSNZGI2IQGEyelX/rTjN79Kh7wcnqKZ8KvBH404jXBZyUWyIzswmo\naluiU1lFeT3wUETcMvINSXflF8nMbPxqXpMzi3J/RNzT5r0P5hHGzGyi6j5SzjrQd4uk8yXtKdyS\npkq6Hrgi/2hmZt3r5dkXkuZLukvSNkkXjPL+UZK+K2mTpB9KeslE82cV5RnAscAmSX2SlgK3AuuA\nkye6YzOzPPTq7AtJk4CrgTOAE4AFkk4Ysdq/A1+OiOcArwcGJpq/bfsiInYAi9NivAa4D5gVEcMT\n3amZWV562L04CdgWET8FkPRF4CzgzhG7OzB9/ASSOjkhbUfKkqZIWgYsBOYDNwCrJPVNdKdmZnmJ\nLpbWayrSpb9lU4cDrcfVhtPXWl0MvFHSMLASmPA87VkH+jaSDMWXRMROYLWk6cCApO0RsWCiOzcz\n67VuDvT9/TUVexltsqiRW18AXBMRH5N0CnCdpGdExK7OU/y9rKI8Z2SrIiI2A7MlLRrvDs3M8tTD\n9sUwcGTL8yPYuz3xZpJOAhGxTtL+wBOBB8a707bti6zecUQsH+8Ozczy1MPLrG8Djpd0jKTJJAfy\nRl7p/AvgdABJ04D9gV9PJL/vEmdmjdKr85QjYqektwPfAiYBn4mILZIuAYYiYgXwHmC5pHeRDNLP\njZhYgtyLsj6c9x7GFh8qO0Fi2YayE8DlLyo7QeKYT5SdAM7ziZ2N1MtrRyJiJckBvNbXLmp5fCfw\nvB7u0iNlM2uWul/R56JsZo1S85rsomxmzeKRsplZhVT15vWdclE2s0apeU12UTazZnH7wsysQmpe\nk12UzaxZPFI2M6uQmtdkF2UzaxaffWFmViFuX5iZVUjNa3LmHH1IOlDSsaO8/qz8IpmZjV8vJ04t\nQ9Z0UK8FfgR8RdIWSc9tefuarI22TrHC0NbeJDUz60A300FVUdZI+d+AGRExnWSevuskvTJ9b7Rp\nUvaIiMGImBkRM5k5rUdRzczG1sOb3Jciq6e8b0TcDxARt0p6AXCTpCOo7i8ZM3uUq2pbolNZI+U/\ntPaT0wI9l2SK7afnnMvMbFzq3r7IGim/nxFtioj4o6T5wAdyTWVmNk5NHilfC7xK0p7CLWkq8Fng\nzLyDmZmNR2PPvgBmAMcAmyT1SVoK3AqsAzy7mZlVUmPbFxGxA3hrWozXAPcBsyJiuKhwZmbdqupZ\nFZ3KOk95iqRlJKfDzQduAFZJ6isqnJlZtxo7UgY2AgPAkojYCayWNB0YkLQ9IhYUktDMrAtV7RV3\nKqsozxnZqoiIzcBsSYvyjWVmNj41r8mZPeW2veOIWJ5PHDOziWnySNnMrHZqXpPzL8qLTsx7D2Nb\ntqHsBInFM8pOAG/4StkJEleeUXYC2PZg2QksD3U/+8IjZTNrFLcvzMwqpOY12UXZzJrFI2Uzswqp\neU12UTazZvGBPjOzCnH7wsysQmpek7NnszYzq5te3k9Z0nxJd0naJumCjPVeLSkkzZxofhdlM2uU\nXt0lTtIk4GrgDOAEYIGkE0ZZ7/HAecAPepHfRdnMGqWHI+WTgG0R8dOI+AvwRZI5Skf6CHAZ8HAv\n8rsom1mj7IrOlzEcDtzT8nw4fW0PSc8BjoyIm3qV30XZzBqlm/aFpH5JQy1Lf8umxN72lHJJ+wAf\nB97Ty/yZZ19IOhQgIn4p6UnAqcBdEbGllyHMzHqlm1PiImIQGGzz9jBwZMvzI0imxdvt8cAzgLWS\nAA4FVkh6eUQMdRH572RNB7WYZJLU9ZL+FbgJeBnwVUlvztpo62+frd/cOt5sZmZd6+F0ULcBx0s6\nRtJk4PXAij37ifh9RDwxIo6OiKOB9cCECjJkj5TfDjwdOADYDhyXjpgPAr4LfLrdF7b+9um/cXHd\nTxs0sxrp1cUjEbFT0tuBbwGTgM9ExBZJlwBDEbEiewvjk1WUH4mIh4CHJP0kIn6ZBt0hyYXWzCqp\nl8UpIlYCK0e8dlGbdef2Yp9ZRXmXpMdExCPAS3e/KGl/fIDQzCqqyfe+eAXpL50R8/UdAtyQZygz\ns/Gq+70vska8twDvlrSncEuaCnwUeHnewczMxqOXl1mXIasozwCOBTZJ6pO0FLiV5IyMk4sIZ2bW\nrR6efVGKtu2LiNgBLE6L8RqS8/NmjWhlmJlVSlVHwJ3KOk95iqRlwEJgPkkfeZWkvqLCmZl1a1cX\nSxVlHejbCAwASyJiJ7Ba0nRgQNL2iFhQSEIzsy7UfaScVZTnjGxVRMRmYLakRfnGMjMbn5rX5Mye\nctvecUQszyeOmdnENHmkbGZWOzWvyfkX5S9X4H5yl7+o7ASJN3yl7ATwhVeVnSAx6ZKyE8CFc8pO\nYHnwSNnMrEKafJm1mVnt1LwmuyibWbO4fWFmViE1r8kuymbWLB4pm5lViA/0mZlVSM1rsouymTWL\n2xdmZhVS85rsomxmzeKRsplZhdS8Jrsom1mz1P3si6w5+vYi6T/zCmJm1gt1nzi17UhZ0pUjXwLe\nJOlxABFxXsbX9gP9AAe84lQmnzStB1HNzMZW0VrbsayR8iuBg4EhYEP65yPp4w1ZG42IwYiYGREz\nXZDNrEh1HylnFeVpwG9IJk1dExHXAn+MiGvTx2ZmlRNdLFWUNR3UH4F3SpoBXC/pG3TZgzYzK1pV\nR8CdaltkJR0JEBEbgD7gz8D30/dOLSSdmVmXdkXnSxVljXxvkXS+pH0jcTXwXknXA1cUlM/MrCtN\n7inPAI4FNknqk7QUWA+sA04uIpyZWbea3FPeASxOi/Ea4D5gVkQMFxXOzKxbVR0BdyqrpzxF0jJg\nIckZGDcAqyT1FRXOzKxbjR0pAxuBAWBJROwEVkuaDgxI2h4RCwpJaGbWhaoewOtUVlGeM7JVERGb\ngdmSFuUby8xsfGpek9u3L7J6xxGxPJ84ZmYT08uzLyTNl3SXpG2SLhjl/f0kfSl9/weSjp5ofl8M\nYmaN0quesqRJwNXAGcAJwAJJJ4xY7c3Ajog4Dvg48NGJ5ndRNrNG6eFI+SRgW0T8NCL+AnwROGvE\nOmcBu287cQNwuiRNJH/u91P+8Tvy3sPYjvlE2QkSV55RdgKYdEnZCRJ/vajsBHDx2rITWB562FM+\nHLin5fkwe1+jsWediNgp6ffAIST3DRoXj5TNrFG6ucxaUr+koZalv2VTo414R9b8TtbpimceMbNG\n6ebikYgYBAbbvD0MHNny/AiSi+hGW2dY0r7AE4AHO0+wN4+UzaxRenjxyG3A8ZKOkTQZeD2wYsQ6\nK4Bz0sevBr4TMbFrCj1SNrNG6dVl1mmP+O3At4BJwGciYoukS4ChiFgBfBq4TtI2khHy6ye6Xxdl\nM2uUXl48EhErgZUjXruo5fHDwGt6uEsXZTNrlrrfkMhF2cwapcn3vjAzq52a12QXZTNrFrcvzMwq\npOY12UXZzJrlUTNSlnQM8Bzgzoj4UX6RzMzGr+4H+rKmg/pay+OzgO8AZwJfl3Ru/tHMzLpX9+mg\nsi6zfmrL4/cDfRGxEHge8K6sjbbe5ONzn9nag5hmZp3p5U3uy5DVvmiNvG9E/AwgIn4jaVfWRltv\n8vHA/y2u6F/dzJqo7gUnqyg/W9IfSG5Nt5+kQyPil+mNOSYVE8/MrDtVHQF3KqsoHxMRvxjl9QOA\nJTnlMTObkJrX5Mye8lpJ56f3CAVA0lSSOauuyD2Zmdk47NrV+VJFWUV5BnAssElSn6SlwK3AOvae\nEsXMrBLqfvZF2/ZFROwAFqfFeA3JHfdnRcRwUeHMzLpV955y1nnKUyQtAxYC80lmal0lqa+ocGZm\n3WrsSBnYCAwASyJiJ7Ba0nRgQNL2iFhQSEIzsy5Utdh2KqsozxnZqoiIzcBsSYvyjWVmNj51b19k\n9ZTb9o4jYnk+cczMJqbu977wXeLMrFFqXpNdlM2sWRrbvuiVgdvy3sPYzqvIWdXbHiw7AVw4p+wE\niYvXlp0ALp5bdoJEFT6LJql5TfZI2cyaxSNlM7MK8YE+M7MKqXlNdlE2s2Zx+8LMrEJqXpNdlM2s\nWTxSNjOrkJrXZBdlM2sWn31hZlYhbl+YmVVIzWuyi7KZNUtjR8qSjgIeiIiHJQk4FzgRuBNYnt74\n3sysUmpekzMnTl3Z8v6lwEuBHwDPBQZzzmVmNi67ovOlirKK8j4R8VD6+IXAayPi+oj4F5KZrtuS\n1C9pSNLQhhu39iqrmdmYIjpfJkLSwZK+Lenu9M+DMtY9UNK9kq4aa7tZRfmelklSfw4cmW78kLE2\nGhGDETEzImbOOHPaWKubmfVMgROnXgDcHBHHAzenz9v5CHBLJxvNKspvAS6U9D1gMrBZ0neANcC7\nO4psZlawokbKwFnAtenja4GzR1tJ0gxgKrC6k41mnn0RES+QNA14GnANMAzcBjyvo8hmZgXrptZK\n6gf6W14ajIhOj5lNjYj7ASLifklPHmX7+wAfA94EnN7JRrOK8i2SPglcERFb0x1MBT4H/CPJAT8z\ns0rpZgScFuC2RVjSGuDQUd76YIe7eBuwMiLuSU5iG1tWUZ5BctbFJklLgWeStC0uA/65w0BmZoXq\n5VkVEfHCdu9J+pWkw9JR8mHAA6OsdgpwqqS3AY8DJkv6U0S07T+3LcoRsQNYnBbkNcB9wKyIGO7w\n72NmVrgCLx5ZAZxDMng9B/j63lnin3Y/lnQuMDOrIEPGgT5JUyQtAxYC84EbgFUtZ2SYmVVOgWdf\nXArMk3Q3MC99jqSZkj413o1mtS82AgPAkvTqvdWSpgMDkrZHxILx7tTMLC9FDZQj4reMcvAuIoZI\nzl4b+fo1JCdMZMoqynNGtioiYjMwW9KisTZsZlaGxt77Iqt3HBHL84ljZjYxNa/JvkucmTVLVe9p\n0SkXZTNrlMa2L8ya7uK1ZSdIXDy37ASwfGPZCXqn5jXZRdnMmsUjZTOzCql5TXZRNrNm8YE+M7MK\ncfvCzKxCal6TXZTNrFk8UjYzq5Ca12QXZTNrFo+UzcwqxGdfmJlVSM1rsouymTWL2xdmZhVS85qc\nOR3UyyXtX2QYM7OJiuh8qaK2RRn4EjAs6TpJL5E0qdONSuqXNCRpaMONWyee0sysQwXO0ZeLrKL8\nI+B44HvAe4D7JH1S0mljbTQiBiNiZkTMnHHmtB5FNTMb267ofKmirKIcEbEjIpZHxOnAs4E7gUsl\n3VNMPDOz7jS5faHWJxHxy4i4MiJOAZ6fbywzs/Fpcvviioz3jup1EDOzXmjySPnDks6XtOe0OUlT\nJV1PdsE2MytNk0fKJwLHApsk9UlaCtwKrANOLiKcmVm3du3qfKmithePRMTvgMVpMV4D3AfMiojh\nosKZmXWrqiPgTmVdPDJF0jJgITAfuAFYJamvqHBmZt2qe/si6zLrjcAAsCQidgKrJU0HBiRtj4gF\nhSQ0M+tCVQ/gdSqrKM8Z2aqIiM3AbEmL8o1lZjY+Na/JmT3ltr3jiFieTxwzs4lp8kjZzKx2qnr5\ndKdclM1Ktnxj2Qlg0YllJ+idmtdkF2Uzaxa3L8zMKqTmNTnzij4zs9op6t4Xkg6W9G1Jd6d/HtRm\nvcskbZG0VdKVkjTaeru5KJtZoxR48cgFwM0RcTxwc/r870iaDTwPeBbwDOC5QOY96V2UzaxRCrzJ\n/VnAtenja4GzR1kngP2BycB+wGOAX2Vt1D1lM2uUAg/0TY2I+5N9xv2Snrx3llgn6bvA/ST3qL8q\nIjLnyHNRNrNG6aYmS+oH+lteGoyIwZb31wCHjvKlH+xw+8cB04Aj0pe+LWlORHyv3de4KJtZo3Qz\nUk4L8GDG+y9s956kX0k6LB0lHwY8MMpqrwDWR8Sf0q9ZBcwimft0VO4pm1mjFHigbwVwTvr4HODr\no6zzC+A0SftKegzJQb7M9oWLspk1SoEH+i4F5km6G5iXPkfSTEmfSte5AfgJcAdwO3B7RNyYtVG3\nL8ysUYo60BcRvwVOH+X1IeAt6eO/Aou72a6Lspk1St2v6MssypLmAL+KiLskPZ+kQb01Ir5RSDoz\nsy7V/d4XWdNBfYKkR3KdpI8AlwEHAO+SdHnWRiX1SxqSNLThxsyetplZTzV5Oqh5JJcFHgDcCxwe\nEQ9JuhTYBLyv3Re2nmZy8drFVf27m1kD1X2knFWUIyJC0u6JuHf/VXfhszbMrKKafJP7b0j6Psn1\n2p8CvixpPTAXuKWAbGZmXat5Tc4sylcBXyMZMa+XdCzJ1SnLSa7jNjOrnLq3L7LaELcApwJDABHx\nE+A64OXAFflHMzPrXt0P9GUV5RnAPwCbJPVJWgrcCqwDTi4inJlZt4q6yX1e2rYvImIH8Na0GK8B\n7gNmRcRwUeHMzLpV0VrbsazzlKdIWgYsBOaTXMO9SlJfUeHMzLpV4L0vcpF1oG8jMAAsiYidwGpJ\n04EBSdsjYkEhCc3MulDVtkSnsorynJGtiojYDMyWtCjfWGZm41PzmpzZU27bO46I5fnEMTObmCaP\nlM3MaqfmNdlF2cyapaoH8DqlqMFYX1J/62SGj9YMVclRhQxVyVGFDFXJUYUMTVCXGwv1j71K7qqQ\nAaqRowoZoBo5qpABqpGjChlqry5F2czsUcFF2cysQupSlKvQp6pCBqhGjipkgGrkqEIGqEaOKmSo\nvVoc6DMze7Soy0jZzOxRoVJFWdKRkn4m6eD0+UHp86dK+qak30m6qaQMp0laJ2mLpB9Kel1JOZ4q\naYOkzWmWt5aRIX1+oKR7JV2VV4axckj6a/pZbJa0oqQMR0laLWmrpDslHV1whnNaPoPNkh6WdHYe\nGcbI8VRJl6U/l1slXSlJeeVorIio1AKcDwymj5cBH0gfnw6cCdxURgbgacDx6WtPIZl9ZUoJOSYD\n+6WvPQ74OfCUor8f6fP/Br4AXFXiz8Wf8t53BxnWAvNaviePLeP7kb52MPBgnhna5QBmA/8LTEqX\ndcDcor4/TVlKDzDKN/sxwA+BdwJbgMkt780tqCi3zdCyzu27i3RZOYBDgF/kXJRHzUAyCcIXgXML\nKsrtchRZlPfKAJwAfL/MDCPe7wc+X9JncQqwATgAeCzJrEXTivpsmrKUHqDNN/zFJJewzxvxeiFF\nOStD+t5JwFZgnzJyAEem/yAeIrm1aqEZSNpea9MchRTljM9iZ/qPfz1wdgmfxdnATcBXgU3A5cCk\noj+Hlve+A7ysxO/HfwG/A34P/EcROZq2VKqn3OIMkvbAM6qWQdJhJHMVLoyIXWXkiIh7IuJZwHHA\nOZKmFpzhbcDKiLgn5/2OlQPgqIiYCbwB+EQ6wW+RGfYlmcvyvcBzSaZQO7fgDMCen81nAt/Kef+j\n5pB0HDANOAI4HOiTNKegLM1R9m+FUX77Tif579BRJP81P6zlvbkU074YNQNwIMnN/19T9mfRss5n\ngVcXmQH4fPr458BvgD8Al1bgs7imhM9iFrC2ZZ03AVeX8TkAS0n7vGX8bALvAy5sWeci4Pwi8jRp\nKT3AiG+0SA4O7P6v4Tto6Y8VUZTbZSDpmd0MvLPMz4JkFHJA+tpBwI+BZ5bx/UhfO5ec2xcZn8VB\n/O2g5xOBu4ETCs4wieT4wpPS1z9LTi2lDv59rAdeUOLP5utI5vPcl6TnfDNwZt55mraUHmDEN7sf\n+FLL80kkBw5OA/4H+DXwZ2AYeHHBGT4EPAJsblmml/BZfIikn3x7+md/Gd+PlteKKMpZPxd3pJ/F\nHcCbS8owL/1e3EEyWt/rwHABGY4G7qWY4xxZOZaRHG+5E7gi7yxNXHxFn5lZhVT1QJ+Z2aOSi7KZ\nWYW4KJuZVYiLsplZhbgom5lViIuymVmFuCibmVWIi7KZWYX8Pz/eqscAxSbeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7c9cd6be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(data.drop(['Y1','Y2'],axis=1).corr(),cmap='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop(['Y1','Y2','X1','X2','X4'],axis=1)\n",
    "y1 = data['Y1']\n",
    "y2 = data['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7c9cd6898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7c9d05940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(X.corr(),cmap='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating train and test datasets\n",
    "X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaling the data with minmax scaler\n",
    "min_max = MinMaxScaler()\n",
    "X_train_scaled = min_max.fit_transform(X_train)\n",
    "X_test_scaled = min_max.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best N: {'n_neighbors': 17}\n",
      "best cross validation score : 0.9140\n",
      "Train r2: 0.919\n",
      "Test r2: 0.912\n"
     ]
    }
   ],
   "source": [
    "knnreg1 = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {'n_neighbors':[1,5,7,11,15,17,21]}\n",
    "grid_search = GridSearchCV(knnreg1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best N: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "knnreg1 = KNeighborsRegressor(n_neighbors = grid_search.best_params_['n_neighbors'])\n",
    "knnreg1.fit(X_train_scaled, y1_train)\n",
    "knnreg1_pred_y1train = knnreg1.predict(X_train_scaled)\n",
    "knnreg1_pred_y1test = knnreg1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, knnreg1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, knnreg1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.9116\n",
      "Bagging Train r2: 0.922\n",
      "Bagging Test r2: 0.919\n",
      "Pasting Train r2: 0.919\n",
      "Pasting Test r2: 0.912\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_knnreg1 = BaggingRegressor(knnreg1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_knnreg1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.4f}'.format(bag_knnreg1.oob_score_))\n",
    "bag_knnreg1_pred_y1train = bag_knnreg1.predict(X_train_scaled)\n",
    "bag_knnreg1_pred_y1test = bag_knnreg1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_knnreg1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_knnreg1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_knnreg1 = BaggingRegressor(knnreg1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_knnreg1.fit(X_train_scaled,y1_train)\n",
    "paste_knnreg1_pred_y1train = paste_knnreg1.predict(X_train_scaled)\n",
    "paste_knnreg1_pred_y1test = paste_knnreg1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_knnreg1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_knnreg1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.9220\n",
      "Boosting Train r2: 0.94\n",
      "Boosting Test r2: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_knnreg1 = AdaBoostRegressor(base_estimator = knnreg1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_knnreg1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_knnreg1 = AdaBoostRegressor(base_estimator = knnreg1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_knnreg1.fit(X_train_scaled, y1_train)\n",
    "adaboost_knnreg1_pred_y1train = adaboost_knnreg1.predict(X_train_scaled)\n",
    "adaboost_knnreg1_pred_y1test = adaboost_knnreg1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.2f}'.format(r2_score(y1_train, adaboost_knnreg1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.2f}'.format(r2_score(y1_test, adaboost_knnreg1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best N: {'n_neighbors': 21}\n",
      "best cross validation score : 0.890\n",
      "Train r2: 0.902\n",
      "Test r2: 0.901\n"
     ]
    }
   ],
   "source": [
    "knnreg2 = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {'n_neighbors':[1,5,7,11,15,17,21]}\n",
    "grid_search = GridSearchCV(knnreg2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best N: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "knnreg2 = KNeighborsRegressor(n_neighbors = grid_search.best_params_['n_neighbors'])\n",
    "knnreg2.fit(X_train_scaled, y2_train)\n",
    "knnreg2_pred_y2train = knnreg2.predict(X_train_scaled)\n",
    "knnreg2_pred_y2test = knnreg2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, knnreg2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, knnreg2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.8887\n",
      "Bagging Train r2: 0.900\n",
      "Bagging Test r2: 0.900\n",
      "Pasting Train r2: 0.902\n",
      "Pasting Test r2: 0.901\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_knnreg2 = BaggingRegressor(knnreg2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_knnreg2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.4f}'.format(bag_knnreg2.oob_score_))\n",
    "bag_knnreg2_pred_y2train = bag_knnreg2.predict(X_train_scaled)\n",
    "bag_knnreg2_pred_y2test = bag_knnreg2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_knnreg2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_knnreg2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_knnreg2 = BaggingRegressor(knnreg2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_knnreg2.fit(X_train_scaled,y2_train)\n",
    "paste_knnreg2_pred_y2train = paste_knnreg2.predict(X_train_scaled)\n",
    "paste_knnreg2_pred_y2test = paste_knnreg2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_knnreg2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_knnreg2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.892\n",
      "Boosting Train r2: 0.902\n",
      "Boosting Test r2: 0.905\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_knnreg2 = AdaBoostRegressor(base_estimator = knnreg2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_knnreg2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_knnreg2 = AdaBoostRegressor(base_estimator = knnreg2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_knnreg2.fit(X_train_scaled, y2_train)\n",
    "adaboost_knnreg2_pred_y2train = adaboost_knnreg2.predict(X_train_scaled)\n",
    "adaboost_knnreg2_pred_y2test = adaboost_knnreg2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_knnreg2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_knnreg2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.910\n",
      "Test r2: 0.916\n"
     ]
    }
   ],
   "source": [
    "lm1 = LinearRegression()\n",
    "lm1.fit(X_train_scaled, y1_train)\n",
    "lm1_pred_y1train = lm1.predict(X_train_scaled)\n",
    "lm1_pred_y1test = lm1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, lm1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, lm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.909\n",
      "Bagging Train r2: 0.910\n",
      "Bagging Test r2: 0.915\n",
      "Pasting Train r2: 0.910\n",
      "Pasting Test r2: 0.916\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_lm1 = BaggingRegressor(lm1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_lm1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_lm1.oob_score_))\n",
    "bag_lm1_pred_y1train = bag_lm1.predict(X_train_scaled)\n",
    "bag_lm1_pred_y1test = bag_lm1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_lm1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_lm1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_lm1 = BaggingRegressor(lm1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_lm1.fit(X_train_scaled,y1_train)\n",
    "paste_lm1_pred_y1train = paste_lm1.predict(X_train_scaled)\n",
    "paste_lm1_pred_y1test = paste_lm1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_lm1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_lm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.907\n",
      "Boosting Train r2: 0.909\n",
      "Boosting Test r2: 0.913\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_lm1 = AdaBoostRegressor(base_estimator = lm1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_lm1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_lm1 = AdaBoostRegressor(base_estimator = lm1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_lm1.fit(X_train_scaled, y1_train)\n",
    "adaboost_lm1_pred_y1train = adaboost_lm1.predict(X_train_scaled)\n",
    "adaboost_lm1_pred_y1test = adaboost_lm1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_lm1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_lm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.877\n",
      "Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "lm2 = LinearRegression()\n",
    "lm2.fit(X_train_scaled, y2_train)\n",
    "lm2_pred_y2train = lm2.predict(X_train_scaled)\n",
    "lm2_pred_y2test = lm2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, lm2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, lm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.875\n",
      "Bagging Train r2: 0.877\n",
      "Bagging Test r2: 0.895\n",
      "Pasting Train r2: 0.877\n",
      "Pasting Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_lm2 = BaggingRegressor(lm2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_lm2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_lm2.oob_score_))\n",
    "bag_lm2_pred_y2train = bag_lm2.predict(X_train_scaled)\n",
    "bag_lm2_pred_y2test = bag_lm2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_lm2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_lm2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_lm2 = BaggingRegressor(lm2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_lm2.fit(X_train_scaled,y2_train)\n",
    "paste_lm2_pred_y2train = paste_lm2.predict(X_train_scaled)\n",
    "paste_lm2_pred_y2test = paste_lm2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_lm2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_lm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.870\n",
      "Boosting Train r2: 0.873\n",
      "Boosting Test r2: 0.896\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_lm2 = AdaBoostRegressor(base_estimator = lm2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_lm2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_lm2 = AdaBoostRegressor(base_estimator = lm2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_lm2.fit(X_train_scaled, y2_train)\n",
    "adaboost_lm2_pred_y2train = adaboost_lm2.predict(X_train_scaled)\n",
    "adaboost_lm2_pred_y2test = adaboost_lm2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_lm2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_lm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: {'alpha': 0.05}\n",
      "best cross validation score : 0.909\n",
      "Train r2: 0.910\n",
      "Test r2: 0.916\n"
     ]
    }
   ],
   "source": [
    "ridge1 = Ridge()\n",
    "\n",
    "param_grid = {'alpha':[0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(ridge1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best alpha: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "ridge1 = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "ridge1.fit(X_train_scaled, y1_train)\n",
    "ridge1_pred_y1train = ridge1.predict(X_train_scaled)\n",
    "ridge1_pred_y1test = ridge1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, ridge1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, ridge1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.9086\n",
      "Bagging Train r2: 0.91\n",
      "Bagging Test r2: 0.92\n",
      "Pasting Train r2: 0.91\n",
      "Pasting Test r2: 0.92\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_ridge1 = BaggingRegressor(ridge1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_ridge1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.4f}'.format(bag_ridge1.oob_score_))\n",
    "bag_ridge1_pred_y1train = bag_ridge1.predict(X_train_scaled)\n",
    "bag_ridge1_pred_y1test = bag_ridge1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.2f}'.format(r2_score(y1_train, bag_ridge1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.2f}'.format(r2_score(y1_test, bag_ridge1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_ridge1 = BaggingRegressor(ridge1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_ridge1.fit(X_train_scaled,y1_train)\n",
    "paste_ridge1_pred_y1train = paste_ridge1.predict(X_train_scaled)\n",
    "paste_ridge1_pred_y1test = paste_ridge1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.2f}'.format(r2_score(y1_train, paste_ridge1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.2f}'.format(r2_score(y1_test, paste_ridge1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.907\n",
      "Boosting Train r2: 0.909\n",
      "Boosting Test r2: 0.913\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_ridge1 = AdaBoostRegressor(base_estimator = ridge1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_ridge1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_ridge1 = AdaBoostRegressor(base_estimator = ridge1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_ridge1.fit(X_train_scaled, y1_train)\n",
    "adaboost_ridge1_pred_y1train = adaboost_ridge1.predict(X_train_scaled)\n",
    "adaboost_ridge1_pred_y1test = adaboost_ridge1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_ridge1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_ridge1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: {'alpha': 0.05}\n",
      "best cross validation score : 0.875\n",
      "Train r2: 0.877\n",
      "Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "ridge2 = Ridge()\n",
    "\n",
    "param_grid = {'alpha':[0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(ridge2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best alpha: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "ridge2 = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "ridge2.fit(X_train_scaled, y2_train)\n",
    "ridge2_pred_y2train = ridge2.predict(X_train_scaled)\n",
    "ridge2_pred_y2test = ridge2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, ridge2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, ridge2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.875\n",
      "Bagging Train r2: 0.877\n",
      "Bagging Test r2: 0.895\n",
      "Pasting Train r2: 0.877\n",
      "Pasting Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_ridge2 = BaggingRegressor(ridge2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_ridge2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_ridge2.oob_score_))\n",
    "bag_ridge2_pred_y2train = bag_ridge2.predict(X_train_scaled)\n",
    "bag_ridge2_pred_y2test = bag_ridge2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_ridge2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_ridge2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_ridge2 = BaggingRegressor(ridge2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_ridge2.fit(X_train_scaled,y2_train)\n",
    "paste_ridge2_pred_y2train = paste_ridge2.predict(X_train_scaled)\n",
    "paste_ridge2_pred_y2test = paste_ridge2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_ridge2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_ridge2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.870\n",
      "Boosting Train r2: 0.874\n",
      "Boosting Test r2: 0.896\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_ridge2 = AdaBoostRegressor(base_estimator = ridge2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_ridge2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_ridge2 = AdaBoostRegressor(base_estimator = ridge2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_ridge2.fit(X_train_scaled, y2_train)\n",
    "adaboost_ridge2_pred_y2train = adaboost_ridge2.predict(X_train_scaled)\n",
    "adaboost_ridge2_pred_y2test = adaboost_ridge2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_ridge2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_ridge2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: {'alpha': 0.01}\n",
      "best cross validation score : 0.909\n",
      "Train r2: 0.910\n",
      "Test r2: 0.916\n"
     ]
    }
   ],
   "source": [
    "lasso1 = Lasso()\n",
    "\n",
    "param_grid = {'alpha':[0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(lasso1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best alpha: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "lasso1 = Lasso(alpha=grid_search.best_params_['alpha'])\n",
    "lasso1.fit(X_train_scaled, y1_train)\n",
    "lasso1_pred_y1train = lasso1.predict(X_train_scaled)\n",
    "lasso1_pred_y1test = lasso1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, lasso1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, lasso1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.909\n",
      "Bagging Train r2: 0.910\n",
      "Bagging Test r2: 0.916\n",
      "Pasting Train r2: 0.910\n",
      "Pasting Test r2: 0.916\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_lasso1 = BaggingRegressor(lasso1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_lasso1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_lasso1.oob_score_))\n",
    "bag_lasso1_pred_y1train = bag_lasso1.predict(X_train_scaled)\n",
    "bag_lasso1_pred_y1test = bag_lasso1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_lasso1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_lasso1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_lasso1 = BaggingRegressor(lasso1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_lasso1.fit(X_train_scaled,y1_train)\n",
    "paste_lasso1_pred_y1train = paste_lasso1.predict(X_train_scaled)\n",
    "paste_lasso1_pred_y1test = paste_lasso1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_lasso1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_lasso1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.908\n",
      "Boosting Train r2: 0.909\n",
      "Boosting Test r2: 0.912\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_lasso1 = AdaBoostRegressor(base_estimator = lasso1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_lasso1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_lasso1 = AdaBoostRegressor(base_estimator = lasso1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_lasso1.fit(X_train_scaled, y1_train)\n",
    "adaboost_lasso1_pred_y1train = adaboost_lasso1.predict(X_train_scaled)\n",
    "adaboost_lasso1_pred_y1test = adaboost_lasso1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_lasso1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_lasso1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: {'alpha': 0.01}\n",
      "best cross validation score : 0.875\n",
      "Train r2: 0.877\n",
      "Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "lasso2 = Lasso()\n",
    "\n",
    "param_grid = {'alpha':[0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(lasso2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best alpha: {}'.format(grid_search.best_params_))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "lasso2 = Lasso(alpha=grid_search.best_params_['alpha'])\n",
    "lasso2.fit(X_train_scaled, y2_train)\n",
    "lasso2_pred_y2train = lasso2.predict(X_train_scaled)\n",
    "lasso2_pred_y2test = lasso2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, lasso2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, lasso2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.875\n",
      "Bagging Train r2: 0.877\n",
      "Bagging Test r2: 0.895\n",
      "Pasting Train r2: 0.877\n",
      "Pasting Test r2: 0.895\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_lasso2 = BaggingRegressor(lasso2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_lasso2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_lasso2.oob_score_))\n",
    "bag_lasso2_pred_y2train = bag_lasso2.predict(X_train_scaled)\n",
    "bag_lasso2_pred_y2test = bag_lasso2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_lasso2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_lasso2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_lasso2 = BaggingRegressor(lasso2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_lasso2.fit(X_train_scaled,y2_train)\n",
    "paste_lasso2_pred_y2train = paste_lasso2.predict(X_train_scaled)\n",
    "paste_lasso2_pred_y2test = paste_lasso2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_lasso2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_lasso2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.871\n",
      "Boosting Train r2: 0.874\n",
      "Boosting Test r2: 0.896\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_lasso2 = AdaBoostRegressor(base_estimator = lasso2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_lasso2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_lasso2 = AdaBoostRegressor(base_estimator = lasso2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_lasso2.fit(X_train_scaled, y2_train)\n",
    "adaboost_lasso2_pred_y2train = adaboost_lasso2.predict(X_train_scaled)\n",
    "adaboost_lasso2_pred_y2test = adaboost_lasso2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_lasso2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_lasso2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C: 100\n",
      "best epsilon: 0.05\n",
      "best cross validation score : 0.908\n",
      "Train r2: 0.908\n",
      "Test r2: 0.919\n"
     ]
    }
   ],
   "source": [
    "svm1 = SVR(kernel = 'linear')\n",
    "param_grid = {'C': [0.1,0.5,1,5,10,50,100],\n",
    "              'epsilon': [0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(svm1, param_grid, cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best C: {}'.format(grid_search.best_params_['C']))\n",
    "print('best epsilon: {}'.format(grid_search.best_params_['epsilon']))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "svm1 = SVR(kernel = 'linear',epsilon=grid_search.best_params_['epsilon'],C=grid_search.best_params_['C'])\n",
    "svm1.fit(X_train_scaled, y1_train)\n",
    "svm1_pred_y1train = svm1.predict(X_train_scaled)\n",
    "svm1_pred_y1test = svm1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, svm1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, svm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.908\n",
      "Bagging Train r2: 0.909\n",
      "Bagging Test r2: 0.918\n",
      "Pasting Train r2: 0.908\n",
      "Pasting Test r2: 0.919\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_svm1 = BaggingRegressor(svm1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_svm1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_svm1.oob_score_))\n",
    "bag_svm1_pred_y1train = bag_svm1.predict(X_train_scaled)\n",
    "bag_svm1_pred_y1test = bag_svm1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_svm1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_svm1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_svm1 = BaggingRegressor(svm1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_svm1.fit(X_train_scaled,y1_train)\n",
    "paste_svm1_pred_y1train = paste_svm1.predict(X_train_scaled)\n",
    "paste_svm1_pred_y1test = paste_svm1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_svm1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_svm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 1.0\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.910\n",
      "Boosting Train r2: 0.910\n",
      "Boosting Test r2: 0.914\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_svm1 = AdaBoostRegressor(base_estimator = svm1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_svm1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_svm1 = AdaBoostRegressor(base_estimator = svm1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_svm1.fit(X_train_scaled, y1_train)\n",
    "adaboost_svm1_pred_y1train = adaboost_svm1.predict(X_train_scaled)\n",
    "adaboost_svm1_pred_y1test = adaboost_svm1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_svm1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_svm1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C: 50\n",
      "best epsilon: 1\n",
      "best cross validation score : 0.875\n",
      "Train r2: 0.876\n",
      "Test r2: 0.892\n"
     ]
    }
   ],
   "source": [
    "svm2 = SVR(kernel = 'linear')\n",
    "param_grid = {'C': [0.1,0.5,1,5,10,50,100],\n",
    "              'epsilon': [0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(svm2, param_grid, cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best C: {}'.format(grid_search.best_params_['C']))\n",
    "print('best epsilon: {}'.format(grid_search.best_params_['epsilon']))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "svm2 = SVR(kernel = 'linear',epsilon=grid_search.best_params_['epsilon'],C=grid_search.best_params_['C'])\n",
    "svm2.fit(X_train_scaled, y2_train)\n",
    "svm2_pred_y2train = svm2.predict(X_train_scaled)\n",
    "svm2_pred_y2test = svm2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, svm2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, svm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.874\n",
      "Bagging Train r2: 0.876\n",
      "Bagging Test r2: 0.891\n",
      "Pasting Train r2: 0.876\n",
      "Pasting Test r2: 0.892\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_svm2 = BaggingRegressor(svm2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_svm2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_svm2.oob_score_))\n",
    "bag_svm2_pred_y2train = bag_svm2.predict(X_train_scaled)\n",
    "bag_svm2_pred_y2test = bag_svm2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_svm2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_svm2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_svm2 = BaggingRegressor(svm2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_svm2.fit(X_train_scaled,y2_train)\n",
    "paste_svm2_pred_y2train = paste_svm2.predict(X_train_scaled)\n",
    "paste_svm2_pred_y2test = paste_svm2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_svm2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_svm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 100\n",
      "best train-set score : 0.875\n",
      "Boosting Train r2: 0.877\n",
      "Boosting Test r2: 0.896\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_svm2 = AdaBoostRegressor(base_estimator = svm2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_svm2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_svm2 = AdaBoostRegressor(base_estimator = svm2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_svm2.fit(X_train_scaled, y2_train)\n",
    "adaboost_svm2_pred_y2train = adaboost_svm2.predict(X_train_scaled)\n",
    "adaboost_svm2_pred_y2test = adaboost_svm2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_svm2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_svm2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C: 100\n",
      "best epsilon: 1\n",
      "best cross validation score : 0.9369\n",
      "Train r2: 0.909\n",
      "Test r2: 0.918\n"
     ]
    }
   ],
   "source": [
    "svm_rbf1 = SVR(kernel = 'rbf')\n",
    "param_grid = {'C': [0.1,0.5,1,5,10,50,100],\n",
    "              'epsilon': [0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(svm_rbf1, param_grid, cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best C: {}'.format(grid_search.best_params_['C']))\n",
    "print('best epsilon: {}'.format(grid_search.best_params_['epsilon']))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "svm_rbf1 = SVR(kernel = 'linear',epsilon=grid_search.best_params_['epsilon'],C=grid_search.best_params_['C'])\n",
    "svm_rbf1.fit(X_train_scaled, y1_train)\n",
    "svm_rbf1_pred_y1train = svm_rbf1.predict(X_train_scaled)\n",
    "svm_rbf1_pred_y1test = svm_rbf1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, svm_rbf1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, svm_rbf1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.907\n",
      "Bagging Train r2: 0.909\n",
      "Bagging Test r2: 0.918\n",
      "Pasting Train r2: 0.909\n",
      "Pasting Test r2: 0.918\n"
     ]
    }
   ],
   "source": [
    "#Bagging\n",
    "bag_svm_rbf1 = BaggingRegressor(svm_rbf1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_svm_rbf1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_svm_rbf1.oob_score_))\n",
    "bag_svm_rbf1_pred_y1train = bag_svm_rbf1.predict(X_train_scaled)\n",
    "bag_svm_rbf1_pred_y1test = bag_svm_rbf1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_svm_rbf1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_svm_rbf1_pred_y1test)))\n",
    "\n",
    "#Pasting\n",
    "paste_svm_rbf1 = BaggingRegressor(svm_rbf1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_svm_rbf1.fit(X_train_scaled,y1_train)\n",
    "paste_svm_rbf1_pred_y1train = paste_svm_rbf1.predict(X_train_scaled)\n",
    "paste_svm_rbf1_pred_y1test = paste_svm_rbf1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_svm_rbf1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_svm_rbf1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.5\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.909\n",
      "Boosting Train r2: 0.910\n",
      "Boosting Test r2: 0.914\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_svm_rbf1 = AdaBoostRegressor(base_estimator = svm_rbf1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_svm_rbf1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_svm_rbf1 = AdaBoostRegressor(base_estimator = svm_rbf1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_svm_rbf1.fit(X_train_scaled, y1_train)\n",
    "adaboost_svm_rbf1_pred_y1train = adaboost_svm_rbf1.predict(X_train_scaled)\n",
    "adaboost_svm_rbf1_pred_y1test = adaboost_svm_rbf1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_svm_rbf1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_svm_rbf1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C: 100\n",
      "best epsilon: 1\n",
      "best cross validation score : 0.902\n",
      "Train r2: 0.907\n",
      "Test r2: 0.917\n"
     ]
    }
   ],
   "source": [
    "svm_rbf2 = SVR(kernel = 'rbf')\n",
    "param_grid = {'C': [0.1,0.5,1,5,10,50,100],\n",
    "              'epsilon': [0.01,0.05,1,5,10,50,100]}\n",
    "grid_search = GridSearchCV(svm_rbf2, param_grid, cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best C: {}'.format(grid_search.best_params_['C']))\n",
    "print('best epsilon: {}'.format(grid_search.best_params_['epsilon']))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "svm_rbf2 = SVR(kernel = 'rbf',epsilon=grid_search.best_params_['epsilon'],C=grid_search.best_params_['C'])\n",
    "svm_rbf2.fit(X_train_scaled, y2_train)\n",
    "svm_rbf2_pred_y2train = svm_rbf2.predict(X_train_scaled)\n",
    "svm_rbf2_pred_y2test = svm_rbf2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, svm_rbf2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, svm_rbf2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.903\n",
      "Bagging Train r2: 0.907\n",
      "Bagging Test r2: 0.916\n",
      "Pasting Train r2: 0.907\n",
      "Pasting Test r2: 0.917\n"
     ]
    }
   ],
   "source": [
    "#bagging\n",
    "bag_svm_rbf2 = BaggingRegressor(svm_rbf2, bootstrap = True, n_estimators=100,oob_score=True)\n",
    "bag_svm_rbf2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_svm_rbf2.oob_score_))\n",
    "bag_svm_rbf2_pred_y2train = bag_svm_rbf2.predict(X_train_scaled)\n",
    "bag_svm_rbf2_pred_y2test = bag_svm_rbf2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_svm_rbf2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_svm_rbf2_pred_y2test)))\n",
    "\n",
    "#Pasting\n",
    "paste_svm_rbf2 = BaggingRegressor(svm_rbf2, bootstrap = False, n_estimators=100)\n",
    "paste_svm_rbf2.fit(X_train_scaled,y2_train)\n",
    "paste_svm_rbf2_pred_y2train = paste_svm_rbf2.predict(X_train_scaled)\n",
    "paste_svm_rbf2_pred_y2test = paste_svm_rbf2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_svm_rbf2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_svm_rbf2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 100\n",
      "best train-set score : 0.906\n",
      "Boosting Train r2: 0.915\n",
      "Boosting Test r2: 0.925\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "adaboost_svm_rbf2 = AdaBoostRegressor(base_estimator = svm_rbf2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_svm_rbf2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_svm_rbf2 = AdaBoostRegressor(base_estimator = svm_rbf2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_svm_rbf2.fit(X_train_scaled, y2_train)\n",
    "adaboost_svm_rbf2_pred_y2train = adaboost_svm_rbf2.predict(X_train_scaled)\n",
    "adaboost_svm_rbf2_pred_y2test = adaboost_svm_rbf2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_svm_rbf2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_svm_rbf2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.979\n",
      "Train r2: 0.982\n",
      "Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "tree1 = DecisionTreeRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(tree1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "tree1 = DecisionTreeRegressor(max_depth=grid_search.best_params_['max_depth'])\n",
    "tree1.fit(X_train_scaled, y1_train)\n",
    "tree1_pred_y1train = tree1.predict(X_train_scaled)\n",
    "tree1_pred_y1test = tree1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, tree1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, tree1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.9791\n",
      "Bagging Train r2: 0.982\n",
      "Bagging Test r2: 0.979\n",
      "Pasting Train r2: 0.982\n",
      "Pasting Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "bag_tree1 = BaggingRegressor(tree1, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_tree1.fit(X_train_scaled,y1_train)\n",
    "print('Out of bag score: {0:0.4f}'.format(bag_tree1.oob_score_))\n",
    "bag_tree1_pred_y1train = bag_tree1.predict(X_train_scaled)\n",
    "bag_tree1_pred_y1test = bag_tree1.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y1_train, bag_tree1_pred_y1train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y1_test, bag_tree1_pred_y1test)))\n",
    "\n",
    "paste_tree1 = BaggingRegressor(tree1, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_tree1.fit(X_train_scaled,y1_train)\n",
    "paste_tree1_pred_y1train = paste_tree1.predict(X_train_scaled)\n",
    "paste_tree1_pred_y1test = paste_tree1.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y1_train, paste_tree1_pred_y1train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y1_test, paste_tree1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 50\n",
      "best train-set score : 0.979\n",
      "Boosting Train r2: 0.982\n",
      "Boosting Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "adaboost_tree1 = AdaBoostRegressor(base_estimator = tree1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_tree1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_tree1 = AdaBoostRegressor(base_estimator = tree1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_tree1.fit(X_train_scaled, y1_train)\n",
    "adaboost_tree1_pred_y1train = adaboost_tree1.predict(X_train_scaled)\n",
    "adaboost_tree1_pred_y1test = adaboost_tree1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_tree1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_tree1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.9434\n",
      "Train r2: 0.954\n",
      "Test r2: 0.957\n"
     ]
    }
   ],
   "source": [
    "tree2 = DecisionTreeRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(tree2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "tree2 = DecisionTreeRegressor(max_depth=grid_search.best_params_['max_depth'])\n",
    "tree2.fit(X_train_scaled, y2_train)\n",
    "tree2_pred_y2train = tree2.predict(X_train_scaled)\n",
    "tree2_pred_y2test = tree2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, tree2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, tree2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of bag score: 0.945\n",
      "Bagging Train r2: 0.955\n",
      "Bagging Test r2: 0.958\n",
      "Pasting Train r2: 0.954\n",
      "Pasting Test r2: 0.957\n"
     ]
    }
   ],
   "source": [
    "bag_tree2 = BaggingRegressor(tree2, bootstrap = True, n_estimators=100,oob_score=True,random_state=101)\n",
    "bag_tree2.fit(X_train_scaled,y2_train)\n",
    "print('Out of bag score: {0:0.3f}'.format(bag_tree2.oob_score_))\n",
    "bag_tree2_pred_y2train = bag_tree2.predict(X_train_scaled)\n",
    "bag_tree2_pred_y2test = bag_tree2.predict(X_test_scaled)\n",
    "print('Bagging Train r2: {:.3f}'.format(r2_score(y2_train, bag_tree2_pred_y2train)))\n",
    "print('Bagging Test r2: {:.3f}'.format(r2_score(y2_test, bag_tree2_pred_y2test)))\n",
    "\n",
    "paste_tree2 = BaggingRegressor(tree2, bootstrap = False, n_estimators=100,random_state=101)\n",
    "paste_tree2.fit(X_train_scaled,y2_train)\n",
    "paste_tree2_pred_y2train = paste_tree2.predict(X_train_scaled)\n",
    "paste_tree2_pred_y2test = paste_tree2.predict(X_test_scaled)\n",
    "print('Pasting Train r2: {:.3f}'.format(r2_score(y2_train, paste_tree2_pred_y2train)))\n",
    "print('Pasting Test r2: {:.3f}'.format(r2_score(y2_test, paste_tree2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 200\n",
      "best train-set score : 0.947\n",
      "Boosting Train r2: 0.955\n",
      "Boosting Test r2: 0.959\n"
     ]
    }
   ],
   "source": [
    "adaboost_tree2 = AdaBoostRegressor(base_estimator = tree2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_tree2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_tree2 = AdaBoostRegressor(base_estimator = tree2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_tree2.fit(X_train_scaled, y2_train)\n",
    "adaboost_tree2_pred_y2train = adaboost_tree2.predict(X_train_scaled)\n",
    "adaboost_tree2_pred_y2test = adaboost_tree2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_tree2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_tree2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.9783\n",
      "Train r2: 0.982\n",
      "Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "forest1 = RandomForestRegressor(n_estimators = 100,bootstrap=True, oob_score=True,random_state=101)\n",
    "\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(forest1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "forest1 = RandomForestRegressor(n_estimators = 100, max_depth=grid_search.best_params_['max_depth'],random_state=101)\n",
    "forest1.fit(X_train_scaled, y1_train)\n",
    "forest1_pred_y1train = forest1.predict(X_train_scaled)\n",
    "forest1_pred_y1test = forest1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, forest1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, forest1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.9787\n",
      "Train r2: 0.982\n",
      "Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "forest1 = RandomForestRegressor(n_estimators = 100,bootstrap=False,random_state=101)\n",
    "\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(forest1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "forest1 = RandomForestRegressor(n_estimators = 100, max_depth=grid_search.best_params_['max_depth'],random_state=101)\n",
    "forest1.fit(X_train_scaled, y1_train)\n",
    "forest1_pred_y1train = forest1.predict(X_train_scaled)\n",
    "forest1_pred_y1test = forest1.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y1_train, forest1_pred_y1train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y1_test, forest1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 100\n",
      "best train-set score : 0.979\n",
      "Boosting Train r2: 0.982\n",
      "Boosting Test r2: 0.979\n"
     ]
    }
   ],
   "source": [
    "adaboost_forest1 = AdaBoostRegressor(base_estimator = forest1,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_forest1, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y1_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_forest1 = AdaBoostRegressor(base_estimator = forest1, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_forest1.fit(X_train_scaled, y1_train)\n",
    "adaboost_forest1_pred_y1train = adaboost_forest1.predict(X_train_scaled)\n",
    "adaboost_forest1_pred_y1test = adaboost_forest1.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y1_train, adaboost_forest1_pred_y1train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y1_test, adaboost_forest1_pred_y1test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.946\n",
      "Train r2: 0.955\n",
      "Test r2: 0.958\n"
     ]
    }
   ],
   "source": [
    "forest2 = RandomForestRegressor(n_estimators = 100,bootstrap=True, oob_score=True,random_state=101)\n",
    "\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(forest2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "forest2 = RandomForestRegressor(n_estimators = 100, max_depth=grid_search.best_params_['max_depth'],random_state=101)\n",
    "forest2.fit(X_train_scaled, y2_train)\n",
    "forest2_pred_y2train = forest2.predict(X_train_scaled)\n",
    "forest2_pred_y2test = forest2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, forest2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, forest2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max_depth: 5\n",
      "best cross validation score : 0.9434\n",
      "Train r2: 0.955\n",
      "Test r2: 0.958\n"
     ]
    }
   ],
   "source": [
    "forest2 = RandomForestRegressor(n_estimators = 100,bootstrap=False,random_state=101)\n",
    "\n",
    "param_grid = {'max_depth': [2,3,4,5]}\n",
    "grid_search = GridSearchCV(forest2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best max_depth: {}'.format(grid_search.best_params_['max_depth']))\n",
    "print('best cross validation score : {0:0.4f}'.format(grid_search.best_score_))\n",
    "\n",
    "forest2 = RandomForestRegressor(n_estimators = 100, max_depth=grid_search.best_params_['max_depth'],random_state=101)\n",
    "forest2.fit(X_train_scaled, y2_train)\n",
    "forest2_pred_y2train = forest2.predict(X_train_scaled)\n",
    "forest2_pred_y2test = forest2.predict(X_test_scaled)\n",
    "print('Train r2: {:.3f}'.format(r2_score(y2_train, forest2_pred_y2train)))\n",
    "print('Test r2: {:.3f}'.format(r2_score(y2_test, forest2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning_rate: 0.1\n",
      "best n_estimators: 100\n",
      "best train-set score : 0.947\n",
      "Boosting Train r2: 0.956\n",
      "Boosting Test r2: 0.959\n"
     ]
    }
   ],
   "source": [
    "adaboost_forest2 = AdaBoostRegressor(base_estimator = forest2,random_state=101)\n",
    "param_grid = {'learning_rate': [0.1,0.5,1.0],\n",
    "              'n_estimators': [50,100,200]}\n",
    "grid_search = GridSearchCV(adaboost_forest2, param_grid, cv = 5,return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y2_train)\n",
    "print('best learning_rate: {}'.format(grid_search.best_params_['learning_rate']))\n",
    "print('best n_estimators: {}'.format(grid_search.best_params_['n_estimators']))\n",
    "print('best train-set score : {0:0.3f}'.format(grid_search.best_score_))\n",
    "\n",
    "adaboost_forest2 = AdaBoostRegressor(base_estimator = forest2, learning_rate = grid_search.best_params_['learning_rate'], \n",
    "                                     n_estimators =grid_search.best_params_['n_estimators'], random_state=101)\n",
    "adaboost_forest2.fit(X_train_scaled, y2_train)\n",
    "adaboost_forest2_pred_y2train = adaboost_forest2.predict(X_train_scaled)\n",
    "adaboost_forest2_pred_y2test = adaboost_forest2.predict(X_test_scaled)\n",
    "print('Boosting Train r2: {:.3f}'.format(r2_score(y2_train, adaboost_forest2_pred_y2train)))\n",
    "print('Boosting Test r2: {:.3f}'.format(r2_score(y2_test, adaboost_forest2_pred_y2test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.concat([y1,y2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y['total'] = y['Y1']+y['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d7ce0764a8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIAo9EVEAkShLyISIAp9EZEA+f8cfyVuva9+gAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7cc6bc6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y['total'],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def efficiency(col):\n",
    "    if col <= 40:\n",
    "        return('High')\n",
    "    elif (col > 40) & (col <=75):\n",
    "        return('Average')\n",
    "    else:\n",
    "        return('Low')\n",
    "    \n",
    "y['efficiency'] = y['total'].apply(efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = y['efficiency']\n",
    "label = pd.get_dummies(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, label_train, label_test = train_test_split(X, label, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=5, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(4,activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(3,activation='sigmoid', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "691/691 [==============================] - 0s 514us/step - loss: 0.6697 - acc: 0.5822\n",
      "Epoch 2/300\n",
      "691/691 [==============================] - 0s 91us/step - loss: 0.5933 - acc: 0.6541\n",
      "Epoch 3/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5744 - acc: 0.6667\n",
      "Epoch 4/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5738 - acc: 0.6667\n",
      "Epoch 5/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5736 - acc: 0.6667\n",
      "Epoch 6/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5733 - acc: 0.6667\n",
      "Epoch 7/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5733 - acc: 0.6667\n",
      "Epoch 8/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5737 - acc: 0.6739\n",
      "Epoch 9/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5729 - acc: 0.6667\n",
      "Epoch 10/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5730 - acc: 0.6667\n",
      "Epoch 11/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5725 - acc: 0.6667\n",
      "Epoch 12/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5727 - acc: 0.6667\n",
      "Epoch 13/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5727 - acc: 0.6667\n",
      "Epoch 14/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5728 - acc: 0.6667\n",
      "Epoch 15/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5722 - acc: 0.6667\n",
      "Epoch 16/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5718 - acc: 0.6667\n",
      "Epoch 17/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5720 - acc: 0.6667\n",
      "Epoch 18/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5717 - acc: 0.6667\n",
      "Epoch 19/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5720 - acc: 0.6667\n",
      "Epoch 20/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5724 - acc: 0.6681\n",
      "Epoch 21/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.5719 - acc: 0.6667\n",
      "Epoch 22/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5713 - acc: 0.6667\n",
      "Epoch 23/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5711 - acc: 0.6667\n",
      "Epoch 24/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5709 - acc: 0.6667\n",
      "Epoch 25/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5716 - acc: 0.6744\n",
      "Epoch 26/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.5711 - acc: 0.6667\n",
      "Epoch 27/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5705 - acc: 0.6667\n",
      "Epoch 28/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5709 - acc: 0.6667\n",
      "Epoch 29/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5709 - acc: 0.6667\n",
      "Epoch 30/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5710 - acc: 0.6753\n",
      "Epoch 31/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5705 - acc: 0.6676\n",
      "Epoch 32/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5700 - acc: 0.6667\n",
      "Epoch 33/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5706 - acc: 0.6667\n",
      "Epoch 34/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5701 - acc: 0.6667\n",
      "Epoch 35/300\n",
      "691/691 [==============================] - 0s 112us/step - loss: 0.5702 - acc: 0.6734\n",
      "Epoch 36/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.5696 - acc: 0.6734\n",
      "Epoch 37/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5695 - acc: 0.6667\n",
      "Epoch 38/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.5698 - acc: 0.6758\n",
      "Epoch 39/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5694 - acc: 0.6686\n",
      "Epoch 40/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5692 - acc: 0.7014\n",
      "Epoch 41/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5687 - acc: 0.6667\n",
      "Epoch 42/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5689 - acc: 0.6667\n",
      "Epoch 43/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5691 - acc: 0.6942\n",
      "Epoch 44/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5692 - acc: 0.6667\n",
      "Epoch 45/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5688 - acc: 0.6763\n",
      "Epoch 46/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5686 - acc: 0.6744\n",
      "Epoch 47/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5687 - acc: 0.6715\n",
      "Epoch 48/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5684 - acc: 0.6860\n",
      "Epoch 49/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5682 - acc: 0.6696\n",
      "Epoch 50/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5679 - acc: 0.7125\n",
      "Epoch 51/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5678 - acc: 0.6807\n",
      "Epoch 52/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5677 - acc: 0.6874\n",
      "Epoch 53/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.5681 - acc: 0.6807\n",
      "Epoch 54/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5676 - acc: 0.6715\n",
      "Epoch 55/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.5676 - acc: 0.7033\n",
      "Epoch 56/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5672 - acc: 0.6787\n",
      "Epoch 57/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5675 - acc: 0.6932\n",
      "Epoch 58/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5671 - acc: 0.6797\n",
      "Epoch 59/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5678 - acc: 0.7067\n",
      "Epoch 60/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5669 - acc: 0.7149\n",
      "Epoch 61/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5670 - acc: 0.6807\n",
      "Epoch 62/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5669 - acc: 0.6739\n",
      "Epoch 63/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5667 - acc: 0.7183\n",
      "Epoch 64/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5659 - acc: 0.6753\n",
      "Epoch 65/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5663 - acc: 0.6778\n",
      "Epoch 66/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5661 - acc: 0.7019\n",
      "Epoch 67/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5663 - acc: 0.6720\n",
      "Epoch 68/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5659 - acc: 0.6782\n",
      "Epoch 69/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5658 - acc: 0.6869\n",
      "Epoch 70/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5653 - acc: 0.7260\n",
      "Epoch 71/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5652 - acc: 0.6778\n",
      "Epoch 72/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.5655 - acc: 0.7038\n",
      "Epoch 73/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5650 - acc: 0.6985\n",
      "Epoch 74/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5644 - acc: 0.7106\n",
      "Epoch 75/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5642 - acc: 0.6696\n",
      "Epoch 76/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5639 - acc: 0.6845\n",
      "Epoch 77/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5638 - acc: 0.6990\n",
      "Epoch 78/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5639 - acc: 0.7057\n",
      "Epoch 79/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5633 - acc: 0.7000\n",
      "Epoch 80/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5631 - acc: 0.7096\n",
      "Epoch 81/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5620 - acc: 0.6985\n",
      "Epoch 82/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5623 - acc: 0.6995\n",
      "Epoch 83/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5616 - acc: 0.6845\n",
      "Epoch 84/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5611 - acc: 0.6840\n",
      "Epoch 85/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5609 - acc: 0.7303\n",
      "Epoch 86/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.5599 - acc: 0.7024\n",
      "Epoch 87/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5593 - acc: 0.6787\n",
      "Epoch 88/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5583 - acc: 0.6768\n",
      "Epoch 89/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5581 - acc: 0.7110\n",
      "Epoch 90/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.5569 - acc: 0.7130\n",
      "Epoch 91/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.5558 - acc: 0.6893\n",
      "Epoch 92/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5541 - acc: 0.7241\n",
      "Epoch 93/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5525 - acc: 0.7139\n",
      "Epoch 94/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.5495 - acc: 0.7125\n",
      "Epoch 95/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5413 - acc: 0.7916\n",
      "Epoch 96/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.5014 - acc: 0.7940\n",
      "Epoch 97/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.4350 - acc: 0.8688\n",
      "Epoch 98/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.3666 - acc: 0.8973\n",
      "Epoch 99/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.3327 - acc: 0.8977\n",
      "Epoch 100/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.3095 - acc: 0.9074\n",
      "Epoch 101/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2883 - acc: 0.9103 0s - loss: 0.2869 - acc: 0.912\n",
      "Epoch 102/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2769 - acc: 0.9093\n",
      "Epoch 103/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2753 - acc: 0.9112\n",
      "Epoch 104/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2720 - acc: 0.9122\n",
      "Epoch 105/300\n",
      "691/691 [==============================] - 0s 97us/step - loss: 0.2698 - acc: 0.9117\n",
      "Epoch 106/300\n",
      "691/691 [==============================] - 0s 97us/step - loss: 0.2653 - acc: 0.9151\n",
      "Epoch 107/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2627 - acc: 0.9170\n",
      "Epoch 108/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2586 - acc: 0.9175\n",
      "Epoch 109/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2557 - acc: 0.9170\n",
      "Epoch 110/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2528 - acc: 0.9214\n",
      "Epoch 111/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2536 - acc: 0.9165\n",
      "Epoch 112/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2515 - acc: 0.9161\n",
      "Epoch 113/300\n",
      "691/691 [==============================] - 0s 96us/step - loss: 0.2487 - acc: 0.9199\n",
      "Epoch 114/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2484 - acc: 0.9190\n",
      "Epoch 115/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2493 - acc: 0.9175\n",
      "Epoch 116/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2471 - acc: 0.9175\n",
      "Epoch 117/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2453 - acc: 0.9185\n",
      "Epoch 118/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2432 - acc: 0.9161\n",
      "Epoch 119/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2420 - acc: 0.9199\n",
      "Epoch 120/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2412 - acc: 0.9199\n",
      "Epoch 121/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2402 - acc: 0.9190\n",
      "Epoch 122/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2394 - acc: 0.9180\n",
      "Epoch 123/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2420 - acc: 0.9151\n",
      "Epoch 124/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2381 - acc: 0.9190\n",
      "Epoch 125/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2405 - acc: 0.9165\n",
      "Epoch 126/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2388 - acc: 0.9180\n",
      "Epoch 127/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2378 - acc: 0.9209\n",
      "Epoch 128/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2381 - acc: 0.9190\n",
      "Epoch 129/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2354 - acc: 0.9185\n",
      "Epoch 130/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2359 - acc: 0.9161\n",
      "Epoch 131/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2338 - acc: 0.9175\n",
      "Epoch 132/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2351 - acc: 0.9214\n",
      "Epoch 133/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2349 - acc: 0.9156\n",
      "Epoch 134/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2343 - acc: 0.9185\n",
      "Epoch 135/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2316 - acc: 0.9214\n",
      "Epoch 136/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2345 - acc: 0.9170\n",
      "Epoch 137/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2309 - acc: 0.9180\n",
      "Epoch 138/300\n",
      "691/691 [==============================] - 0s 97us/step - loss: 0.2301 - acc: 0.9209\n",
      "Epoch 139/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2299 - acc: 0.9180\n",
      "Epoch 140/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2283 - acc: 0.9209\n",
      "Epoch 141/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2283 - acc: 0.9194\n",
      "Epoch 142/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2290 - acc: 0.9194\n",
      "Epoch 143/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2279 - acc: 0.9199\n",
      "Epoch 144/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2275 - acc: 0.9214\n",
      "Epoch 145/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2280 - acc: 0.9165\n",
      "Epoch 146/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2301 - acc: 0.9180\n",
      "Epoch 147/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2264 - acc: 0.9185\n",
      "Epoch 148/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2290 - acc: 0.9170\n",
      "Epoch 149/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2242 - acc: 0.9185\n",
      "Epoch 150/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2258 - acc: 0.9180\n",
      "Epoch 151/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2258 - acc: 0.9165\n",
      "Epoch 152/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2239 - acc: 0.9209\n",
      "Epoch 153/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2256 - acc: 0.9165\n",
      "Epoch 154/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2279 - acc: 0.9165\n",
      "Epoch 155/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2315 - acc: 0.9122\n",
      "Epoch 156/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2246 - acc: 0.9156\n",
      "Epoch 157/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2208 - acc: 0.9209\n",
      "Epoch 158/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2251 - acc: 0.9175\n",
      "Epoch 159/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2248 - acc: 0.9156\n",
      "Epoch 160/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2263 - acc: 0.9219\n",
      "Epoch 161/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2205 - acc: 0.9170\n",
      "Epoch 162/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2239 - acc: 0.9175\n",
      "Epoch 163/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2207 - acc: 0.9156\n",
      "Epoch 164/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2197 - acc: 0.9214\n",
      "Epoch 165/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 102us/step - loss: 0.2192 - acc: 0.9194\n",
      "Epoch 166/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2190 - acc: 0.9209\n",
      "Epoch 167/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2214 - acc: 0.9165\n",
      "Epoch 168/300\n",
      "691/691 [==============================] - 0s 96us/step - loss: 0.2209 - acc: 0.9170\n",
      "Epoch 169/300\n",
      "691/691 [==============================] - 0s 96us/step - loss: 0.2175 - acc: 0.9199\n",
      "Epoch 170/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2208 - acc: 0.9199\n",
      "Epoch 171/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2191 - acc: 0.9165\n",
      "Epoch 172/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2172 - acc: 0.9185\n",
      "Epoch 173/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2190 - acc: 0.9175\n",
      "Epoch 174/300\n",
      "691/691 [==============================] - 0s 101us/step - loss: 0.2179 - acc: 0.9175\n",
      "Epoch 175/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2214 - acc: 0.9185\n",
      "Epoch 176/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2194 - acc: 0.9180\n",
      "Epoch 177/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2182 - acc: 0.9185\n",
      "Epoch 178/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2224 - acc: 0.9170\n",
      "Epoch 179/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2162 - acc: 0.9199\n",
      "Epoch 180/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.2176 - acc: 0.9180\n",
      "Epoch 181/300\n",
      "691/691 [==============================] - 0s 116us/step - loss: 0.2186 - acc: 0.9165\n",
      "Epoch 182/300\n",
      "691/691 [==============================] - 0s 161us/step - loss: 0.2221 - acc: 0.9093\n",
      "Epoch 183/300\n",
      "691/691 [==============================] - 0s 171us/step - loss: 0.2167 - acc: 0.9199\n",
      "Epoch 184/300\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.2145 - acc: 0.9199\n",
      "Epoch 185/300\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.2167 - acc: 0.9190\n",
      "Epoch 186/300\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.2241 - acc: 0.9156\n",
      "Epoch 187/300\n",
      "691/691 [==============================] - 0s 170us/step - loss: 0.2165 - acc: 0.9199\n",
      "Epoch 188/300\n",
      "691/691 [==============================] - 0s 125us/step - loss: 0.2183 - acc: 0.9194\n",
      "Epoch 189/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2188 - acc: 0.9170\n",
      "Epoch 190/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2135 - acc: 0.9204\n",
      "Epoch 191/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2194 - acc: 0.9223\n",
      "Epoch 192/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2167 - acc: 0.9175\n",
      "Epoch 193/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.2165 - acc: 0.9108\n",
      "Epoch 194/300\n",
      "691/691 [==============================] - 0s 115us/step - loss: 0.2155 - acc: 0.9190\n",
      "Epoch 195/300\n",
      "691/691 [==============================] - 0s 115us/step - loss: 0.2154 - acc: 0.9194\n",
      "Epoch 196/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2186 - acc: 0.9190\n",
      "Epoch 197/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2144 - acc: 0.9204\n",
      "Epoch 198/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2151 - acc: 0.9137\n",
      "Epoch 199/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2155 - acc: 0.9214\n",
      "Epoch 200/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2157 - acc: 0.9161\n",
      "Epoch 201/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2142 - acc: 0.9194\n",
      "Epoch 202/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2130 - acc: 0.9190\n",
      "Epoch 203/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2160 - acc: 0.9156\n",
      "Epoch 204/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2203 - acc: 0.9098\n",
      "Epoch 205/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2147 - acc: 0.9238\n",
      "Epoch 206/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2153 - acc: 0.9214\n",
      "Epoch 207/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2220 - acc: 0.9117\n",
      "Epoch 208/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2139 - acc: 0.9228\n",
      "Epoch 209/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2131 - acc: 0.9199\n",
      "Epoch 210/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2172 - acc: 0.9137\n",
      "Epoch 211/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2123 - acc: 0.9243\n",
      "Epoch 212/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.2111 - acc: 0.9199\n",
      "Epoch 213/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.2140 - acc: 0.9214\n",
      "Epoch 214/300\n",
      "691/691 [==============================] - 0s 113us/step - loss: 0.2114 - acc: 0.9146\n",
      "Epoch 215/300\n",
      "691/691 [==============================] - 0s 119us/step - loss: 0.2163 - acc: 0.9103\n",
      "Epoch 216/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2114 - acc: 0.9219\n",
      "Epoch 217/300\n",
      "691/691 [==============================] - 0s 112us/step - loss: 0.2131 - acc: 0.9214\n",
      "Epoch 218/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2133 - acc: 0.9170\n",
      "Epoch 219/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2139 - acc: 0.9199\n",
      "Epoch 220/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2187 - acc: 0.9112\n",
      "Epoch 221/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2115 - acc: 0.9146\n",
      "Epoch 222/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2185 - acc: 0.9199\n",
      "Epoch 223/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2147 - acc: 0.9190\n",
      "Epoch 224/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2153 - acc: 0.9122\n",
      "Epoch 225/300\n",
      "691/691 [==============================] - 0s 120us/step - loss: 0.2134 - acc: 0.9122\n",
      "Epoch 226/300\n",
      "691/691 [==============================] - 0s 112us/step - loss: 0.2148 - acc: 0.9079\n",
      "Epoch 227/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2141 - acc: 0.9194\n",
      "Epoch 228/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2147 - acc: 0.9122\n",
      "Epoch 229/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2126 - acc: 0.9137\n",
      "Epoch 230/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2129 - acc: 0.9204\n",
      "Epoch 231/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2143 - acc: 0.9146\n",
      "Epoch 232/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2111 - acc: 0.9238\n",
      "Epoch 233/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.2102 - acc: 0.9228\n",
      "Epoch 234/300\n",
      "691/691 [==============================] - 0s 113us/step - loss: 0.2111 - acc: 0.9165\n",
      "Epoch 235/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2133 - acc: 0.9175\n",
      "Epoch 236/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2135 - acc: 0.9132\n",
      "Epoch 237/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2158 - acc: 0.9161\n",
      "Epoch 238/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2135 - acc: 0.9180\n",
      "Epoch 239/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2113 - acc: 0.9223\n",
      "Epoch 240/300\n",
      "691/691 [==============================] - 0s 113us/step - loss: 0.2109 - acc: 0.9204\n",
      "Epoch 241/300\n",
      "691/691 [==============================] - 0s 113us/step - loss: 0.2157 - acc: 0.9127\n",
      "Epoch 242/300\n",
      "691/691 [==============================] - 0s 109us/step - loss: 0.2100 - acc: 0.9233\n",
      "Epoch 243/300\n",
      "691/691 [==============================] - 0s 113us/step - loss: 0.2115 - acc: 0.9190\n",
      "Epoch 244/300\n",
      "691/691 [==============================] - 0s 115us/step - loss: 0.2093 - acc: 0.9185\n",
      "Epoch 245/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2106 - acc: 0.9219\n",
      "Epoch 246/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2146 - acc: 0.9141\n",
      "Epoch 247/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 107us/step - loss: 0.2127 - acc: 0.9146\n",
      "Epoch 248/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2105 - acc: 0.9214\n",
      "Epoch 249/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2120 - acc: 0.9165\n",
      "Epoch 250/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2141 - acc: 0.9093\n",
      "Epoch 251/300\n",
      "691/691 [==============================] - 0s 112us/step - loss: 0.2169 - acc: 0.9098\n",
      "Epoch 252/300\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.2114 - acc: 0.9204\n",
      "Epoch 253/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2102 - acc: 0.9238\n",
      "Epoch 254/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2120 - acc: 0.9223\n",
      "Epoch 255/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2140 - acc: 0.9151\n",
      "Epoch 256/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2154 - acc: 0.9137\n",
      "Epoch 257/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2097 - acc: 0.9228\n",
      "Epoch 258/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2108 - acc: 0.9209\n",
      "Epoch 259/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2105 - acc: 0.9204\n",
      "Epoch 260/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2101 - acc: 0.9209\n",
      "Epoch 261/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2098 - acc: 0.9219\n",
      "Epoch 262/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2091 - acc: 0.9243\n",
      "Epoch 263/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2107 - acc: 0.9132\n",
      "Epoch 264/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2105 - acc: 0.9185\n",
      "Epoch 265/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2117 - acc: 0.9219\n",
      "Epoch 266/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2085 - acc: 0.9252\n",
      "Epoch 267/300\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.2119 - acc: 0.9214\n",
      "Epoch 268/300\n",
      "691/691 [==============================] - 0s 118us/step - loss: 0.2089 - acc: 0.9228\n",
      "Epoch 269/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2105 - acc: 0.9185\n",
      "Epoch 270/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2125 - acc: 0.9199\n",
      "Epoch 271/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2107 - acc: 0.9228\n",
      "Epoch 272/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2088 - acc: 0.9180\n",
      "Epoch 273/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2087 - acc: 0.9257\n",
      "Epoch 274/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2099 - acc: 0.9209\n",
      "Epoch 275/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2117 - acc: 0.9122\n",
      "Epoch 276/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2142 - acc: 0.9170\n",
      "Epoch 277/300\n",
      "691/691 [==============================] - 0s 101us/step - loss: 0.2115 - acc: 0.9146\n",
      "Epoch 278/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2088 - acc: 0.9238\n",
      "Epoch 279/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2422 - acc: 0.9108\n",
      "Epoch 280/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2073 - acc: 0.9228\n",
      "Epoch 281/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2092 - acc: 0.9175\n",
      "Epoch 282/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2099 - acc: 0.9151\n",
      "Epoch 283/300\n",
      "691/691 [==============================] - 0s 106us/step - loss: 0.2137 - acc: 0.9180\n",
      "Epoch 284/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2095 - acc: 0.9165\n",
      "Epoch 285/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2091 - acc: 0.9146\n",
      "Epoch 286/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2093 - acc: 0.9165\n",
      "Epoch 287/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2074 - acc: 0.9209\n",
      "Epoch 288/300\n",
      "691/691 [==============================] - 0s 96us/step - loss: 0.2093 - acc: 0.9204\n",
      "Epoch 289/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2076 - acc: 0.9243\n",
      "Epoch 290/300\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.2072 - acc: 0.9194\n",
      "Epoch 291/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2062 - acc: 0.9214\n",
      "Epoch 292/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2146 - acc: 0.9194\n",
      "Epoch 293/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2089 - acc: 0.9252\n",
      "Epoch 294/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2092 - acc: 0.9175\n",
      "Epoch 295/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2135 - acc: 0.9170\n",
      "Epoch 296/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2084 - acc: 0.9247\n",
      "Epoch 297/300\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.2099 - acc: 0.9228\n",
      "Epoch 298/300\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.2071 - acc: 0.9204\n",
      "Epoch 299/300\n",
      "691/691 [==============================] - 0s 100us/step - loss: 0.2102 - acc: 0.9190\n",
      "Epoch 300/300\n",
      "691/691 [==============================] - 0s 99us/step - loss: 0.2084 - acc: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7d4c69cc0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, label_train, epochs = 300, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 247us/step\n",
      "\n",
      "acc: 93.51%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, label_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(predict_y,columns=['y1_pred','y2_pred','y3_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_pred (row):\n",
    "    if row['y1_pred']>0.5:\n",
    "        return 'Average'\n",
    "    elif row['y2_pred']>0.5:\n",
    "        return 'High'\n",
    "    elif row['y3_pred']>0.5:\n",
    "        return 'Low'\n",
    "    \n",
    "predict['label_predicted'] = predict.apply (lambda row: label_pred (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1_pred</th>\n",
       "      <th>y2_pred</th>\n",
       "      <th>y3_pred</th>\n",
       "      <th>label_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.953741</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025971</td>\n",
       "      <td>0.961261</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039470</td>\n",
       "      <td>0.974171</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.801914</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.225270</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710580</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>0.211037</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y1_pred   y2_pred   y3_pred label_predicted\n",
       "0  0.021544  0.953741  0.000245            High\n",
       "1  0.025971  0.961261  0.000385            High\n",
       "2  0.039470  0.974171  0.001070            High\n",
       "3  0.801914  0.007315  0.225270         Average\n",
       "4  0.710580  0.037916  0.211037         Average"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(5, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(3, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model1.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model1.compile(loss='mse', optimizer='adam' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "691/691 [==============================] - 0s 386us/step - loss: 600.2147 - mean_squared_error: 600.2147\n",
      "Epoch 2/200\n",
      "691/691 [==============================] - 0s 42us/step - loss: 597.4614 - mean_squared_error: 597.4614\n",
      "Epoch 3/200\n",
      "691/691 [==============================] - 0s 41us/step - loss: 591.8593 - mean_squared_error: 591.8593\n",
      "Epoch 4/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 580.9231 - mean_squared_error: 580.9231\n",
      "Epoch 5/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 561.4029 - mean_squared_error: 561.4029\n",
      "Epoch 6/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 529.4328 - mean_squared_error: 529.4328\n",
      "Epoch 7/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 482.6427 - mean_squared_error: 482.6427\n",
      "Epoch 8/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 422.3920 - mean_squared_error: 422.3920\n",
      "Epoch 9/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 351.7554 - mean_squared_error: 351.7554\n",
      "Epoch 10/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 276.0636 - mean_squared_error: 276.0636\n",
      "Epoch 11/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 204.3354 - mean_squared_error: 204.3354\n",
      "Epoch 12/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 143.2605 - mean_squared_error: 143.2605\n",
      "Epoch 13/200\n",
      "691/691 [==============================] - 0s 42us/step - loss: 98.1633 - mean_squared_error: 98.1633\n",
      "Epoch 14/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 70.3878 - mean_squared_error: 70.3878\n",
      "Epoch 15/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 55.6982 - mean_squared_error: 55.6982\n",
      "Epoch 16/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 48.9586 - mean_squared_error: 48.9586\n",
      "Epoch 17/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 45.8678 - mean_squared_error: 45.8678\n",
      "Epoch 18/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 44.1346 - mean_squared_error: 44.1346\n",
      "Epoch 19/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 42.8222 - mean_squared_error: 42.8222\n",
      "Epoch 20/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 41.5866 - mean_squared_error: 41.5866\n",
      "Epoch 21/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 40.3498 - mean_squared_error: 40.3498\n",
      "Epoch 22/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 39.1603 - mean_squared_error: 39.1603\n",
      "Epoch 23/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 37.9262 - mean_squared_error: 37.9262\n",
      "Epoch 24/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 36.7499 - mean_squared_error: 36.7499\n",
      "Epoch 25/200\n",
      "691/691 [==============================] - 0s 54us/step - loss: 35.5646 - mean_squared_error: 35.5646\n",
      "Epoch 26/200\n",
      "691/691 [==============================] - 0s 54us/step - loss: 34.4073 - mean_squared_error: 34.4073\n",
      "Epoch 27/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 33.2670 - mean_squared_error: 33.2670\n",
      "Epoch 28/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 32.1396 - mean_squared_error: 32.1396\n",
      "Epoch 29/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 31.0103 - mean_squared_error: 31.0103\n",
      "Epoch 30/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 29.9141 - mean_squared_error: 29.9141\n",
      "Epoch 31/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 28.8287 - mean_squared_error: 28.8287\n",
      "Epoch 32/200\n",
      "691/691 [==============================] - 0s 52us/step - loss: 27.7650 - mean_squared_error: 27.7650\n",
      "Epoch 33/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 26.7550 - mean_squared_error: 26.7550\n",
      "Epoch 34/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 25.7205 - mean_squared_error: 25.7205\n",
      "Epoch 35/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 24.7783 - mean_squared_error: 24.7783\n",
      "Epoch 36/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 23.7816 - mean_squared_error: 23.7816\n",
      "Epoch 37/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 22.8519 - mean_squared_error: 22.8519\n",
      "Epoch 38/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 21.9445 - mean_squared_error: 21.9445\n",
      "Epoch 39/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 21.0924 - mean_squared_error: 21.0924\n",
      "Epoch 40/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 20.2359 - mean_squared_error: 20.2359\n",
      "Epoch 41/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 19.4208 - mean_squared_error: 19.4208\n",
      "Epoch 42/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 18.6281 - mean_squared_error: 18.6281\n",
      "Epoch 43/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 17.8964 - mean_squared_error: 17.8964\n",
      "Epoch 44/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 17.1871 - mean_squared_error: 17.1871\n",
      "Epoch 45/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 16.5274 - mean_squared_error: 16.5274\n",
      "Epoch 46/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 15.9020 - mean_squared_error: 15.9020\n",
      "Epoch 47/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 15.2601 - mean_squared_error: 15.2601\n",
      "Epoch 48/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 14.7117 - mean_squared_error: 14.7117\n",
      "Epoch 49/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 14.1733 - mean_squared_error: 14.1733\n",
      "Epoch 50/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 13.6892 - mean_squared_error: 13.6892\n",
      "Epoch 51/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 13.2266 - mean_squared_error: 13.2266\n",
      "Epoch 52/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 12.8308 - mean_squared_error: 12.8308\n",
      "Epoch 53/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 12.4215 - mean_squared_error: 12.4215\n",
      "Epoch 54/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 12.0699 - mean_squared_error: 12.0699\n",
      "Epoch 55/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.7571 - mean_squared_error: 11.7571\n",
      "Epoch 56/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.4649 - mean_squared_error: 11.4649\n",
      "Epoch 57/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1855 - mean_squared_error: 11.1855\n",
      "Epoch 58/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 10.9475 - mean_squared_error: 10.9475\n",
      "Epoch 59/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 10.7533 - mean_squared_error: 10.7533\n",
      "Epoch 60/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 10.5459 - mean_squared_error: 10.5459\n",
      "Epoch 61/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 10.3826 - mean_squared_error: 10.3826\n",
      "Epoch 62/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 10.2443 - mean_squared_error: 10.2443\n",
      "Epoch 63/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 10.1203 - mean_squared_error: 10.1203\n",
      "Epoch 64/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.9807 - mean_squared_error: 9.9807\n",
      "Epoch 65/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.8864 - mean_squared_error: 9.8864\n",
      "Epoch 66/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.8048 - mean_squared_error: 9.8048\n",
      "Epoch 67/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.7286 - mean_squared_error: 9.7286\n",
      "Epoch 68/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.6604 - mean_squared_error: 9.6604\n",
      "Epoch 69/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.6142 - mean_squared_error: 9.6142\n",
      "Epoch 70/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.5497 - mean_squared_error: 9.5497\n",
      "Epoch 71/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.5602 - mean_squared_error: 9.5602\n",
      "Epoch 72/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.4724 - mean_squared_error: 9.4724\n",
      "Epoch 73/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.4284 - mean_squared_error: 9.4284\n",
      "Epoch 74/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.3952 - mean_squared_error: 9.3952\n",
      "Epoch 75/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.3664 - mean_squared_error: 9.3664\n",
      "Epoch 76/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.3549 - mean_squared_error: 9.3549\n",
      "Epoch 77/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.3545 - mean_squared_error: 9.3545\n",
      "Epoch 78/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.3138 - mean_squared_error: 9.3138\n",
      "Epoch 79/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.3040 - mean_squared_error: 9.3040\n",
      "Epoch 80/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.2742 - mean_squared_error: 9.2742\n",
      "Epoch 81/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.2948 - mean_squared_error: 9.2948\n",
      "Epoch 82/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.2616 - mean_squared_error: 9.2616\n",
      "Epoch 83/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.2417 - mean_squared_error: 9.2417\n",
      "Epoch 84/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.2499 - mean_squared_error: 9.2499\n",
      "Epoch 85/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.2356 - mean_squared_error: 9.2356\n",
      "Epoch 86/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.2543 - mean_squared_error: 9.2543\n",
      "Epoch 87/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.2184 - mean_squared_error: 9.2184\n",
      "Epoch 88/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.2183 - mean_squared_error: 9.2183\n",
      "Epoch 89/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.2349 - mean_squared_error: 9.2349\n",
      "Epoch 90/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.2014 - mean_squared_error: 9.2014\n",
      "Epoch 91/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1985 - mean_squared_error: 9.1985\n",
      "Epoch 92/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.2160 - mean_squared_error: 9.2160\n",
      "Epoch 93/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.2038 - mean_squared_error: 9.2038\n",
      "Epoch 94/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.2261 - mean_squared_error: 9.2261\n",
      "Epoch 95/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1910 - mean_squared_error: 9.1910\n",
      "Epoch 96/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1816 - mean_squared_error: 9.1816\n",
      "Epoch 97/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1952 - mean_squared_error: 9.1952\n",
      "Epoch 98/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1713 - mean_squared_error: 9.1713\n",
      "Epoch 99/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1872 - mean_squared_error: 9.1872\n",
      "Epoch 100/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1651 - mean_squared_error: 9.1651\n",
      "Epoch 101/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1909 - mean_squared_error: 9.1909\n",
      "Epoch 102/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1898 - mean_squared_error: 9.1898\n",
      "Epoch 103/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1716 - mean_squared_error: 9.1716\n",
      "Epoch 104/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 9.1670 - mean_squared_error: 9.1670\n",
      "Epoch 105/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1777 - mean_squared_error: 9.1777\n",
      "Epoch 106/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1901 - mean_squared_error: 9.1901\n",
      "Epoch 107/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1630 - mean_squared_error: 9.1630\n",
      "Epoch 108/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1632 - mean_squared_error: 9.1632\n",
      "Epoch 109/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1559 - mean_squared_error: 9.1559\n",
      "Epoch 110/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1643 - mean_squared_error: 9.1643\n",
      "Epoch 111/200\n",
      "691/691 [==============================] - ETA: 0s - loss: 3.0264 - mean_squared_error: 3.02 - 0s 46us/step - loss: 9.1677 - mean_squared_error: 9.1677\n",
      "Epoch 112/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1828 - mean_squared_error: 9.1828\n",
      "Epoch 113/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1881 - mean_squared_error: 9.1881\n",
      "Epoch 114/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1964 - mean_squared_error: 9.1964\n",
      "Epoch 115/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.2062 - mean_squared_error: 9.2062\n",
      "Epoch 116/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1448 - mean_squared_error: 9.1448\n",
      "Epoch 117/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1628 - mean_squared_error: 9.1628\n",
      "Epoch 118/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1540 - mean_squared_error: 9.1540\n",
      "Epoch 119/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1553 - mean_squared_error: 9.1553\n",
      "Epoch 120/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1750 - mean_squared_error: 9.1750\n",
      "Epoch 121/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.2017 - mean_squared_error: 9.2017\n",
      "Epoch 122/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1625 - mean_squared_error: 9.1625\n",
      "Epoch 123/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.2027 - mean_squared_error: 9.2027\n",
      "Epoch 124/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1764 - mean_squared_error: 9.1764\n",
      "Epoch 125/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1740 - mean_squared_error: 9.1740\n",
      "Epoch 126/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1547 - mean_squared_error: 9.1547\n",
      "Epoch 127/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1508 - mean_squared_error: 9.1508\n",
      "Epoch 128/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1546 - mean_squared_error: 9.1546\n",
      "Epoch 129/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1598 - mean_squared_error: 9.1598\n",
      "Epoch 130/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.2090 - mean_squared_error: 9.2090\n",
      "Epoch 131/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1843 - mean_squared_error: 9.1843\n",
      "Epoch 132/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1599 - mean_squared_error: 9.1599\n",
      "Epoch 133/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1908 - mean_squared_error: 9.1908\n",
      "Epoch 134/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1888 - mean_squared_error: 9.1888\n",
      "Epoch 135/200\n",
      "691/691 [==============================] - ETA: 0s - loss: 13.1886 - mean_squared_error: 13.18 - 0s 49us/step - loss: 9.2315 - mean_squared_error: 9.2315\n",
      "Epoch 136/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1596 - mean_squared_error: 9.1596\n",
      "Epoch 137/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1577 - mean_squared_error: 9.1577\n",
      "Epoch 138/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1799 - mean_squared_error: 9.1799\n",
      "Epoch 139/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1710 - mean_squared_error: 9.1710\n",
      "Epoch 140/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1593 - mean_squared_error: 9.1593\n",
      "Epoch 141/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1778 - mean_squared_error: 9.1778\n",
      "Epoch 142/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1866 - mean_squared_error: 9.1866\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 46us/step - loss: 9.1648 - mean_squared_error: 9.1648\n",
      "Epoch 144/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1592 - mean_squared_error: 9.1592\n",
      "Epoch 145/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1619 - mean_squared_error: 9.1619\n",
      "Epoch 146/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1876 - mean_squared_error: 9.1876\n",
      "Epoch 147/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1617 - mean_squared_error: 9.1617\n",
      "Epoch 148/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1640 - mean_squared_error: 9.1640\n",
      "Epoch 149/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1555 - mean_squared_error: 9.1555\n",
      "Epoch 150/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1833 - mean_squared_error: 9.1833\n",
      "Epoch 151/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1461 - mean_squared_error: 9.1461\n",
      "Epoch 152/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1517 - mean_squared_error: 9.1517\n",
      "Epoch 153/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1827 - mean_squared_error: 9.1827\n",
      "Epoch 154/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1774 - mean_squared_error: 9.1774\n",
      "Epoch 155/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1596 - mean_squared_error: 9.1596\n",
      "Epoch 156/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1610 - mean_squared_error: 9.1610\n",
      "Epoch 157/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1563 - mean_squared_error: 9.1563\n",
      "Epoch 158/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1516 - mean_squared_error: 9.1516\n",
      "Epoch 159/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1419 - mean_squared_error: 9.1419\n",
      "Epoch 160/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1592 - mean_squared_error: 9.1592\n",
      "Epoch 161/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1510 - mean_squared_error: 9.1510\n",
      "Epoch 162/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1787 - mean_squared_error: 9.1787\n",
      "Epoch 163/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1536 - mean_squared_error: 9.1536\n",
      "Epoch 164/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1619 - mean_squared_error: 9.1619\n",
      "Epoch 165/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1572 - mean_squared_error: 9.1572\n",
      "Epoch 166/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1574 - mean_squared_error: 9.1574\n",
      "Epoch 167/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1730 - mean_squared_error: 9.1730\n",
      "Epoch 168/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1686 - mean_squared_error: 9.1686\n",
      "Epoch 169/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1671 - mean_squared_error: 9.1671\n",
      "Epoch 170/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1526 - mean_squared_error: 9.1526\n",
      "Epoch 171/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1526 - mean_squared_error: 9.1526\n",
      "Epoch 172/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1613 - mean_squared_error: 9.1613\n",
      "Epoch 173/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1636 - mean_squared_error: 9.1636\n",
      "Epoch 174/200\n",
      "691/691 [==============================] - ETA: 0s - loss: 7.1073 - mean_squared_error: 7.10 - 0s 48us/step - loss: 9.1723 - mean_squared_error: 9.1723\n",
      "Epoch 175/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1461 - mean_squared_error: 9.1461\n",
      "Epoch 176/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1484 - mean_squared_error: 9.1484\n",
      "Epoch 177/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1880 - mean_squared_error: 9.1880\n",
      "Epoch 178/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1546 - mean_squared_error: 9.1546\n",
      "Epoch 179/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1582 - mean_squared_error: 9.1582\n",
      "Epoch 180/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1528 - mean_squared_error: 9.1528\n",
      "Epoch 181/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1810 - mean_squared_error: 9.1810\n",
      "Epoch 182/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1985 - mean_squared_error: 9.1985\n",
      "Epoch 183/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1521 - mean_squared_error: 9.1521\n",
      "Epoch 184/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1806 - mean_squared_error: 9.1806\n",
      "Epoch 185/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1826 - mean_squared_error: 9.1826\n",
      "Epoch 186/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1565 - mean_squared_error: 9.1565\n",
      "Epoch 187/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1710 - mean_squared_error: 9.1710\n",
      "Epoch 188/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1961 - mean_squared_error: 9.1961\n",
      "Epoch 189/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1500 - mean_squared_error: 9.1500\n",
      "Epoch 190/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1678 - mean_squared_error: 9.1678\n",
      "Epoch 191/200\n",
      "691/691 [==============================] - 0s 49us/step - loss: 9.1552 - mean_squared_error: 9.1552\n",
      "Epoch 192/200\n",
      "691/691 [==============================] - 0s 44us/step - loss: 9.1453 - mean_squared_error: 9.1453\n",
      "Epoch 193/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1572 - mean_squared_error: 9.1572\n",
      "Epoch 194/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1843 - mean_squared_error: 9.1843\n",
      "Epoch 195/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1976 - mean_squared_error: 9.1976\n",
      "Epoch 196/200\n",
      "691/691 [==============================] - 0s 51us/step - loss: 9.1761 - mean_squared_error: 9.1761\n",
      "Epoch 197/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1778 - mean_squared_error: 9.1778\n",
      "Epoch 198/200\n",
      "691/691 [==============================] - 0s 45us/step - loss: 9.1631 - mean_squared_error: 9.1631\n",
      "Epoch 199/200\n",
      "691/691 [==============================] - 0s 46us/step - loss: 9.1600 - mean_squared_error: 9.1600\n",
      "Epoch 200/200\n",
      "691/691 [==============================] - 0s 48us/step - loss: 9.1636 - mean_squared_error: 9.1636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7d569dda0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train_scaled, y1_train, epochs = 200, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 Train r2: 0.910\n",
      "y1 Test r2: 0.915\n"
     ]
    }
   ],
   "source": [
    "y1_train_predict = model1.predict(X_train_scaled)\n",
    "y1_test_predict = model1.predict(X_test_scaled)\n",
    "\n",
    "print('y1 Train r2: {:.3f}'.format(r2_score(y1_train, y1_train_predict)))\n",
    "print('y1 Test r2: {:.3f}'.format(r2_score(y1_test, y1_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(5, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(3, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model2.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model2.compile(loss='mse', optimizer='adam' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "691/691 [==============================] - 0s 411us/step - loss: 695.3715 - mean_squared_error: 695.3715\n",
      "Epoch 2/300\n",
      "691/691 [==============================] - 0s 43us/step - loss: 692.4741 - mean_squared_error: 692.4741\n",
      "Epoch 3/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 686.1369 - mean_squared_error: 686.1369\n",
      "Epoch 4/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 672.7779 - mean_squared_error: 672.7779\n",
      "Epoch 5/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 648.4861 - mean_squared_error: 648.4861\n",
      "Epoch 6/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 610.5732 - mean_squared_error: 610.5732\n",
      "Epoch 7/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 556.8903 - mean_squared_error: 556.8903\n",
      "Epoch 8/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 488.1906 - mean_squared_error: 488.1906\n",
      "Epoch 9/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 408.1896 - mean_squared_error: 408.1896\n",
      "Epoch 10/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 322.9490 - mean_squared_error: 322.9490\n",
      "Epoch 11/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 240.0605 - mean_squared_error: 240.0605\n",
      "Epoch 12/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 168.4434 - mean_squared_error: 168.4434\n",
      "Epoch 13/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 113.4727 - mean_squared_error: 113.4727\n",
      "Epoch 14/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 77.6827 - mean_squared_error: 77.6827\n",
      "Epoch 15/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 57.2102 - mean_squared_error: 57.2102\n",
      "Epoch 16/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 47.7354 - mean_squared_error: 47.7354\n",
      "Epoch 17/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 43.3940 - mean_squared_error: 43.3940\n",
      "Epoch 18/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 41.3554 - mean_squared_error: 41.3554\n",
      "Epoch 19/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 40.0212 - mean_squared_error: 40.0212\n",
      "Epoch 20/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 38.9011 - mean_squared_error: 38.9011\n",
      "Epoch 21/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 37.9014 - mean_squared_error: 37.9014\n",
      "Epoch 22/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 36.8947 - mean_squared_error: 36.8947\n",
      "Epoch 23/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 35.8973 - mean_squared_error: 35.8973\n",
      "Epoch 24/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 34.8863 - mean_squared_error: 34.8863\n",
      "Epoch 25/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 33.9479 - mean_squared_error: 33.9479\n",
      "Epoch 26/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 32.9542 - mean_squared_error: 32.9542\n",
      "Epoch 27/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 32.0527 - mean_squared_error: 32.0527\n",
      "Epoch 28/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 31.0375 - mean_squared_error: 31.0375\n",
      "Epoch 29/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 30.1366 - mean_squared_error: 30.1366\n",
      "Epoch 30/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 29.2242 - mean_squared_error: 29.2242\n",
      "Epoch 31/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 28.3212 - mean_squared_error: 28.3212\n",
      "Epoch 32/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 27.4717 - mean_squared_error: 27.4717\n",
      "Epoch 33/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 26.6007 - mean_squared_error: 26.6007\n",
      "Epoch 34/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 25.7583 - mean_squared_error: 25.7583\n",
      "Epoch 35/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 24.9646 - mean_squared_error: 24.9646\n",
      "Epoch 36/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 24.1535 - mean_squared_error: 24.1535\n",
      "Epoch 37/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 23.4079 - mean_squared_error: 23.4079\n",
      "Epoch 38/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 22.6595 - mean_squared_error: 22.6595\n",
      "Epoch 39/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 21.9415 - mean_squared_error: 21.9415\n",
      "Epoch 40/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 21.2443 - mean_squared_error: 21.2443\n",
      "Epoch 41/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 20.5706 - mean_squared_error: 20.5706\n",
      "Epoch 42/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 19.9544 - mean_squared_error: 19.9544\n",
      "Epoch 43/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 19.3091 - mean_squared_error: 19.3091\n",
      "Epoch 44/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 18.7481 - mean_squared_error: 18.7481\n",
      "Epoch 45/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 18.1505 - mean_squared_error: 18.1505\n",
      "Epoch 46/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 17.6209 - mean_squared_error: 17.6209\n",
      "Epoch 47/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 17.1576 - mean_squared_error: 17.1576\n",
      "Epoch 48/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 16.6542 - mean_squared_error: 16.6542\n",
      "Epoch 49/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 16.1838 - mean_squared_error: 16.1838\n",
      "Epoch 50/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 15.7554 - mean_squared_error: 15.7554\n",
      "Epoch 51/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 15.3657 - mean_squared_error: 15.3657\n",
      "Epoch 52/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 14.9880 - mean_squared_error: 14.9880\n",
      "Epoch 53/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 14.6328 - mean_squared_error: 14.6328\n",
      "Epoch 54/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 14.3147 - mean_squared_error: 14.3147\n",
      "Epoch 55/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 14.0081 - mean_squared_error: 14.0081\n",
      "Epoch 56/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 13.7330 - mean_squared_error: 13.7330\n",
      "Epoch 57/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 13.5092 - mean_squared_error: 13.5092\n",
      "Epoch 58/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 13.2668 - mean_squared_error: 13.2668\n",
      "Epoch 59/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 13.0516 - mean_squared_error: 13.0516\n",
      "Epoch 60/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 12.8480 - mean_squared_error: 12.8480\n",
      "Epoch 61/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 12.6675 - mean_squared_error: 12.6675\n",
      "Epoch 62/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 12.5220 - mean_squared_error: 12.5220\n",
      "Epoch 63/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 12.3847 - mean_squared_error: 12.3847\n",
      "Epoch 64/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 12.2560 - mean_squared_error: 12.2560\n",
      "Epoch 65/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 12.1680 - mean_squared_error: 12.1680\n",
      "Epoch 66/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 12.0600 - mean_squared_error: 12.0600\n",
      "Epoch 67/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.9505 - mean_squared_error: 11.9505\n",
      "Epoch 68/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.8712 - mean_squared_error: 11.8712\n",
      "Epoch 69/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.8038 - mean_squared_error: 11.8038\n",
      "Epoch 70/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.7328 - mean_squared_error: 11.7328\n",
      "Epoch 71/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.6868 - mean_squared_error: 11.6868\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 46us/step - loss: 11.6739 - mean_squared_error: 11.6739\n",
      "Epoch 73/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.6000 - mean_squared_error: 11.6000\n",
      "Epoch 74/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.5481 - mean_squared_error: 11.5481\n",
      "Epoch 75/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.4961 - mean_squared_error: 11.4961\n",
      "Epoch 76/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.4771 - mean_squared_error: 11.4771\n",
      "Epoch 77/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.4462 - mean_squared_error: 11.4462\n",
      "Epoch 78/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.4545 - mean_squared_error: 11.4545\n",
      "Epoch 79/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.4257 - mean_squared_error: 11.4257\n",
      "Epoch 80/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.4247 - mean_squared_error: 11.4247\n",
      "Epoch 81/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.3617 - mean_squared_error: 11.3617\n",
      "Epoch 82/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.3597 - mean_squared_error: 11.3597\n",
      "Epoch 83/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.3435 - mean_squared_error: 11.3435\n",
      "Epoch 84/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.3050 - mean_squared_error: 11.3050\n",
      "Epoch 85/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.3233 - mean_squared_error: 11.3233\n",
      "Epoch 86/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.2956 - mean_squared_error: 11.2956\n",
      "Epoch 87/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.3006 - mean_squared_error: 11.3006\n",
      "Epoch 88/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.2716 - mean_squared_error: 11.2716\n",
      "Epoch 89/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.2482 - mean_squared_error: 11.2482\n",
      "Epoch 90/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.2519 - mean_squared_error: 11.2519\n",
      "Epoch 91/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.2382 - mean_squared_error: 11.2382\n",
      "Epoch 92/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.2419 - mean_squared_error: 11.2419\n",
      "Epoch 93/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.2416 - mean_squared_error: 11.2416\n",
      "Epoch 94/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.2198 - mean_squared_error: 11.2198\n",
      "Epoch 95/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.2152 - mean_squared_error: 11.2152\n",
      "Epoch 96/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1982 - mean_squared_error: 11.1982\n",
      "Epoch 97/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.2243 - mean_squared_error: 11.2243\n",
      "Epoch 98/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.2054 - mean_squared_error: 11.2054\n",
      "Epoch 99/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1887 - mean_squared_error: 11.1887\n",
      "Epoch 100/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1925 - mean_squared_error: 11.1925\n",
      "Epoch 101/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1863 - mean_squared_error: 11.1863\n",
      "Epoch 102/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1792 - mean_squared_error: 11.1792\n",
      "Epoch 103/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1871 - mean_squared_error: 11.1871\n",
      "Epoch 104/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1697 - mean_squared_error: 11.1697\n",
      "Epoch 105/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1822 - mean_squared_error: 11.1822\n",
      "Epoch 106/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1632 - mean_squared_error: 11.1632\n",
      "Epoch 107/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1600 - mean_squared_error: 11.1600\n",
      "Epoch 108/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1630 - mean_squared_error: 11.1630\n",
      "Epoch 109/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1497 - mean_squared_error: 11.1497\n",
      "Epoch 110/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1495 - mean_squared_error: 11.1495\n",
      "Epoch 111/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1565 - mean_squared_error: 11.1565\n",
      "Epoch 112/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1639 - mean_squared_error: 11.1639\n",
      "Epoch 113/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1492 - mean_squared_error: 11.1492\n",
      "Epoch 114/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1833 - mean_squared_error: 11.1833\n",
      "Epoch 115/300\n",
      "691/691 [==============================] - ETA: 0s - loss: 12.9581 - mean_squared_error: 12.95 - 0s 45us/step - loss: 11.1802 - mean_squared_error: 11.1802\n",
      "Epoch 116/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1446 - mean_squared_error: 11.1446\n",
      "Epoch 117/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1393 - mean_squared_error: 11.1393\n",
      "Epoch 118/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1322 - mean_squared_error: 11.1322\n",
      "Epoch 119/300\n",
      "691/691 [==============================] - ETA: 0s - loss: 15.1978 - mean_squared_error: 15.19 - 0s 45us/step - loss: 11.1963 - mean_squared_error: 11.1963\n",
      "Epoch 120/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1398 - mean_squared_error: 11.1398\n",
      "Epoch 121/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1417 - mean_squared_error: 11.1417\n",
      "Epoch 122/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1558 - mean_squared_error: 11.1558\n",
      "Epoch 123/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1738 - mean_squared_error: 11.1738\n",
      "Epoch 124/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1448 - mean_squared_error: 11.1448\n",
      "Epoch 125/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1412 - mean_squared_error: 11.1412\n",
      "Epoch 126/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1382 - mean_squared_error: 11.1382\n",
      "Epoch 127/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1533 - mean_squared_error: 11.1533\n",
      "Epoch 128/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1339 - mean_squared_error: 11.1339\n",
      "Epoch 129/300\n",
      "691/691 [==============================] - ETA: 0s - loss: 10.1188 - mean_squared_error: 10.11 - 0s 48us/step - loss: 11.1208 - mean_squared_error: 11.1208\n",
      "Epoch 130/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1583 - mean_squared_error: 11.1583\n",
      "Epoch 131/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1300 - mean_squared_error: 11.1300\n",
      "Epoch 132/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1860 - mean_squared_error: 11.1860\n",
      "Epoch 133/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1234 - mean_squared_error: 11.1234\n",
      "Epoch 134/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1306 - mean_squared_error: 11.1306\n",
      "Epoch 135/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1531 - mean_squared_error: 11.1531\n",
      "Epoch 136/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1191 - mean_squared_error: 11.1191\n",
      "Epoch 137/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1395 - mean_squared_error: 11.1395\n",
      "Epoch 138/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1387 - mean_squared_error: 11.1387\n",
      "Epoch 139/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1262 - mean_squared_error: 11.1262\n",
      "Epoch 140/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1297 - mean_squared_error: 11.1297\n",
      "Epoch 141/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1193 - mean_squared_error: 11.1193\n",
      "Epoch 142/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1182 - mean_squared_error: 11.1182\n",
      "Epoch 143/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1163 - mean_squared_error: 11.1163\n",
      "Epoch 144/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1087 - mean_squared_error: 11.1087\n",
      "Epoch 145/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1077 - mean_squared_error: 11.1077\n",
      "Epoch 146/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1199 - mean_squared_error: 11.1199\n",
      "Epoch 147/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1640 - mean_squared_error: 11.1640\n",
      "Epoch 148/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1428 - mean_squared_error: 11.1428\n",
      "Epoch 149/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.2439 - mean_squared_error: 11.2439\n",
      "Epoch 150/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1531 - mean_squared_error: 11.1531\n",
      "Epoch 151/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1515 - mean_squared_error: 11.1515\n",
      "Epoch 152/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1140 - mean_squared_error: 11.1140\n",
      "Epoch 153/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1039 - mean_squared_error: 11.1039\n",
      "Epoch 154/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1333 - mean_squared_error: 11.1333\n",
      "Epoch 155/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1853 - mean_squared_error: 11.1853\n",
      "Epoch 156/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1060 - mean_squared_error: 11.1060\n",
      "Epoch 157/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1396 - mean_squared_error: 11.1396\n",
      "Epoch 158/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1273 - mean_squared_error: 11.1273\n",
      "Epoch 159/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1078 - mean_squared_error: 11.1078\n",
      "Epoch 160/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1292 - mean_squared_error: 11.1292\n",
      "Epoch 161/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1330 - mean_squared_error: 11.1330\n",
      "Epoch 162/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1208 - mean_squared_error: 11.1208\n",
      "Epoch 163/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1200 - mean_squared_error: 11.1200\n",
      "Epoch 164/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1364 - mean_squared_error: 11.1364\n",
      "Epoch 165/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1027 - mean_squared_error: 11.1027\n",
      "Epoch 166/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1111 - mean_squared_error: 11.1111\n",
      "Epoch 167/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1367 - mean_squared_error: 11.1367\n",
      "Epoch 168/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1077 - mean_squared_error: 11.1077\n",
      "Epoch 169/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1347 - mean_squared_error: 11.1347\n",
      "Epoch 170/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1343 - mean_squared_error: 11.1343\n",
      "Epoch 171/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1032 - mean_squared_error: 11.1032\n",
      "Epoch 172/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1093 - mean_squared_error: 11.1093\n",
      "Epoch 173/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1120 - mean_squared_error: 11.1120\n",
      "Epoch 174/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1779 - mean_squared_error: 11.1779\n",
      "Epoch 175/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1055 - mean_squared_error: 11.1055\n",
      "Epoch 176/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1249 - mean_squared_error: 11.1249\n",
      "Epoch 177/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1293 - mean_squared_error: 11.1293\n",
      "Epoch 178/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1045 - mean_squared_error: 11.1045\n",
      "Epoch 179/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1868 - mean_squared_error: 11.1868\n",
      "Epoch 180/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1412 - mean_squared_error: 11.1412\n",
      "Epoch 181/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1329 - mean_squared_error: 11.1329\n",
      "Epoch 182/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1038 - mean_squared_error: 11.1038\n",
      "Epoch 183/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1208 - mean_squared_error: 11.1208\n",
      "Epoch 184/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1677 - mean_squared_error: 11.1677\n",
      "Epoch 185/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1096 - mean_squared_error: 11.1096\n",
      "Epoch 186/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1424 - mean_squared_error: 11.1424\n",
      "Epoch 187/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1552 - mean_squared_error: 11.1552\n",
      "Epoch 188/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1414 - mean_squared_error: 11.1414\n",
      "Epoch 189/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1081 - mean_squared_error: 11.1081\n",
      "Epoch 190/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1265 - mean_squared_error: 11.1265\n",
      "Epoch 191/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1114 - mean_squared_error: 11.1114\n",
      "Epoch 192/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1492 - mean_squared_error: 11.1492\n",
      "Epoch 193/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1518 - mean_squared_error: 11.1518\n",
      "Epoch 194/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1352 - mean_squared_error: 11.1352\n",
      "Epoch 195/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1068 - mean_squared_error: 11.1068\n",
      "Epoch 196/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1222 - mean_squared_error: 11.1222\n",
      "Epoch 197/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1324 - mean_squared_error: 11.1324\n",
      "Epoch 198/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1164 - mean_squared_error: 11.1164\n",
      "Epoch 199/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1101 - mean_squared_error: 11.1101\n",
      "Epoch 200/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1263 - mean_squared_error: 11.1263\n",
      "Epoch 201/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1565 - mean_squared_error: 11.1565\n",
      "Epoch 202/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1892 - mean_squared_error: 11.1892\n",
      "Epoch 203/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1084 - mean_squared_error: 11.1084\n",
      "Epoch 204/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1086 - mean_squared_error: 11.1086\n",
      "Epoch 205/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1238 - mean_squared_error: 11.1238\n",
      "Epoch 206/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1342 - mean_squared_error: 11.1342\n",
      "Epoch 207/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1089 - mean_squared_error: 11.1089\n",
      "Epoch 208/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1498 - mean_squared_error: 11.1498\n",
      "Epoch 209/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1299 - mean_squared_error: 11.1299\n",
      "Epoch 210/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1103 - mean_squared_error: 11.1103\n",
      "Epoch 211/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 46us/step - loss: 11.1244 - mean_squared_error: 11.1244\n",
      "Epoch 212/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1172 - mean_squared_error: 11.1172\n",
      "Epoch 213/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1797 - mean_squared_error: 11.1797\n",
      "Epoch 214/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1805 - mean_squared_error: 11.1805\n",
      "Epoch 215/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1103 - mean_squared_error: 11.1103\n",
      "Epoch 216/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1272 - mean_squared_error: 11.1272\n",
      "Epoch 217/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1209 - mean_squared_error: 11.1209\n",
      "Epoch 218/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1189 - mean_squared_error: 11.1189\n",
      "Epoch 219/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1061 - mean_squared_error: 11.1061\n",
      "Epoch 220/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1351 - mean_squared_error: 11.1351\n",
      "Epoch 221/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1435 - mean_squared_error: 11.1435\n",
      "Epoch 222/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1092 - mean_squared_error: 11.1092\n",
      "Epoch 223/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1249 - mean_squared_error: 11.1249\n",
      "Epoch 224/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1212 - mean_squared_error: 11.1212\n",
      "Epoch 225/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1481 - mean_squared_error: 11.1481\n",
      "Epoch 226/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.2097 - mean_squared_error: 11.2097\n",
      "Epoch 227/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1257 - mean_squared_error: 11.1257\n",
      "Epoch 228/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1102 - mean_squared_error: 11.1102\n",
      "Epoch 229/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1158 - mean_squared_error: 11.1158\n",
      "Epoch 230/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1037 - mean_squared_error: 11.1037\n",
      "Epoch 231/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1066 - mean_squared_error: 11.1066\n",
      "Epoch 232/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1407 - mean_squared_error: 11.1407\n",
      "Epoch 233/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1382 - mean_squared_error: 11.1382\n",
      "Epoch 234/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1369 - mean_squared_error: 11.1369\n",
      "Epoch 235/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1689 - mean_squared_error: 11.1689\n",
      "Epoch 236/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1250 - mean_squared_error: 11.1250\n",
      "Epoch 237/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1326 - mean_squared_error: 11.1326\n",
      "Epoch 238/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1462 - mean_squared_error: 11.1462\n",
      "Epoch 239/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1517 - mean_squared_error: 11.1517\n",
      "Epoch 240/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1081 - mean_squared_error: 11.1081\n",
      "Epoch 241/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1529 - mean_squared_error: 11.1529\n",
      "Epoch 242/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1300 - mean_squared_error: 11.1300\n",
      "Epoch 243/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1148 - mean_squared_error: 11.1148\n",
      "Epoch 244/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1088 - mean_squared_error: 11.1088\n",
      "Epoch 245/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1141 - mean_squared_error: 11.1141\n",
      "Epoch 246/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1184 - mean_squared_error: 11.1184\n",
      "Epoch 247/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1393 - mean_squared_error: 11.1393\n",
      "Epoch 248/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1311 - mean_squared_error: 11.1311\n",
      "Epoch 249/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1315 - mean_squared_error: 11.1315\n",
      "Epoch 250/300\n",
      "691/691 [==============================] - 0s 51us/step - loss: 11.1422 - mean_squared_error: 11.1422\n",
      "Epoch 251/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1155 - mean_squared_error: 11.1155\n",
      "Epoch 252/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1121 - mean_squared_error: 11.1121\n",
      "Epoch 253/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1095 - mean_squared_error: 11.1095\n",
      "Epoch 254/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.0962 - mean_squared_error: 11.0962\n",
      "Epoch 255/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1175 - mean_squared_error: 11.1175\n",
      "Epoch 256/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.0941 - mean_squared_error: 11.0941\n",
      "Epoch 257/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1060 - mean_squared_error: 11.1060\n",
      "Epoch 258/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1166 - mean_squared_error: 11.1166\n",
      "Epoch 259/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1009 - mean_squared_error: 11.1009\n",
      "Epoch 260/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1437 - mean_squared_error: 11.1437\n",
      "Epoch 261/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1560 - mean_squared_error: 11.1560\n",
      "Epoch 262/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1063 - mean_squared_error: 11.1063\n",
      "Epoch 263/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1511 - mean_squared_error: 11.1511\n",
      "Epoch 264/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.0996 - mean_squared_error: 11.0996\n",
      "Epoch 265/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.0993 - mean_squared_error: 11.0993\n",
      "Epoch 266/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1076 - mean_squared_error: 11.1076\n",
      "Epoch 267/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1085 - mean_squared_error: 11.1085\n",
      "Epoch 268/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1032 - mean_squared_error: 11.1032\n",
      "Epoch 269/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1378 - mean_squared_error: 11.1378\n",
      "Epoch 270/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1078 - mean_squared_error: 11.1078\n",
      "Epoch 271/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1096 - mean_squared_error: 11.1096\n",
      "Epoch 272/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1067 - mean_squared_error: 11.1067\n",
      "Epoch 273/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1124 - mean_squared_error: 11.1124\n",
      "Epoch 274/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1137 - mean_squared_error: 11.1137\n",
      "Epoch 275/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1181 - mean_squared_error: 11.1181\n",
      "Epoch 276/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1017 - mean_squared_error: 11.1017\n",
      "Epoch 277/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1610 - mean_squared_error: 11.1610\n",
      "Epoch 278/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1190 - mean_squared_error: 11.1190\n",
      "Epoch 279/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1039 - mean_squared_error: 11.1039\n",
      "Epoch 280/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1043 - mean_squared_error: 11.1043\n",
      "Epoch 281/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1134 - mean_squared_error: 11.1134\n",
      "Epoch 282/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1905 - mean_squared_error: 11.1905\n",
      "Epoch 283/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1255 - mean_squared_error: 11.1255\n",
      "Epoch 284/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1357 - mean_squared_error: 11.1357\n",
      "Epoch 285/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1079 - mean_squared_error: 11.1079\n",
      "Epoch 286/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1330 - mean_squared_error: 11.1330\n",
      "Epoch 287/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1696 - mean_squared_error: 11.1696\n",
      "Epoch 288/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1022 - mean_squared_error: 11.1022\n",
      "Epoch 289/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1498 - mean_squared_error: 11.1498\n",
      "Epoch 290/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.3158 - mean_squared_error: 11.3158\n",
      "Epoch 291/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1082 - mean_squared_error: 11.1082\n",
      "Epoch 292/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.0995 - mean_squared_error: 11.0995\n",
      "Epoch 293/300\n",
      "691/691 [==============================] - 0s 44us/step - loss: 11.1337 - mean_squared_error: 11.1337\n",
      "Epoch 294/300\n",
      "691/691 [==============================] - 0s 46us/step - loss: 11.1334 - mean_squared_error: 11.1334\n",
      "Epoch 295/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1104 - mean_squared_error: 11.1104\n",
      "Epoch 296/300\n",
      "691/691 [==============================] - 0s 45us/step - loss: 11.1068 - mean_squared_error: 11.1068\n",
      "Epoch 297/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.0892 - mean_squared_error: 11.0892\n",
      "Epoch 298/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.1577 - mean_squared_error: 11.1577\n",
      "Epoch 299/300\n",
      "691/691 [==============================] - 0s 49us/step - loss: 11.2189 - mean_squared_error: 11.2189\n",
      "Epoch 300/300\n",
      "691/691 [==============================] - 0s 48us/step - loss: 11.1789 - mean_squared_error: 11.1789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7d557fe48>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train_scaled, y2_train, epochs = 300, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2 Train r2: 0.877\n",
      "y2 Test r2: 0.894\n"
     ]
    }
   ],
   "source": [
    "y2_train_predict = model2.predict(X_train_scaled)\n",
    "y2_test_predict = model2.predict(X_test_scaled)\n",
    "\n",
    "print('y2 Train r2: {:.3f}'.format(r2_score(y2_train, y2_train_predict)))\n",
    "print('y2 Test r2: {:.3f}'.format(r2_score(y2_test, y2_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
